{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1704de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import glob\n",
    "import os\n",
    "import sqlalchemy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca03901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_df_memory(df, verbose=False):\n",
    "    \"\"\"\n",
    "    Downcasts numerical columns to smaller, more memory-efficient types.\n",
    "    \"\"\"\n",
    "    initial_mem = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if 'float' in str(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        elif 'int' in str(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "\n",
    "        elif col_type == object:\n",
    "            # Convert string columns to categorical type if they have low cardinality\n",
    "            if df[col].nunique() < len(df) / 20:\n",
    "                df[col] = df[col].astype('category')\n",
    "\n",
    "    final_mem = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    if verbose:\n",
    "        print(f\"Memory reduction: {initial_mem:.2f} MB -> {final_mem:.2f} MB ({100 * (initial_mem - final_mem) / initial_mem:.2f}% saved)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71731ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATABASE CONNECTION & TABLE SETUP ---\n",
    "DB_HOST = \"10.192.5.43\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"ml_db\"\n",
    "DB_PASS = \"pass%401234\"\n",
    "DB_PORT = 5432\n",
    "LOS_TABLE_NAME = \"Ashirvad\"\n",
    "LOS_START_DATE = '2024-01-01'\n",
    "LOS_END_DATE = '2024-12-31'\n",
    "\n",
    "# --- GLOBAL CONSTANTS  ---\n",
    "MAX_SEQUENCE_LENGTH = 8\n",
    "ROLLING_WINDOW_SIZE = 3\n",
    "TARGET_COL = 'NEXT_EMI_LABEL'\n",
    "NEW_LMS_FOLDER_PATH = \"TestData_Oct2025prediction\"\n",
    "GRACE_PERIOD_DAYS = 2 \n",
    "PAID_PERCENTAGE_THRESHOLD = 0.90 \n",
    "TWO_CLASS_STATUS_MAP = {0: 'Paid', 1: 'Not Paid'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89443596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VALIDATION MODE CONSTANT ---\n",
    "TARGET_BACKTEST_EMI = 1  # 1 means we are predicting the last known EMI (EMI N) using data up to EMI N-1\n",
    "\n",
    "# --- FEATURE LISTS (MUST BE EXACT MATCH TO TRAINING) ---\n",
    "# FIX #1: Added RECENT_PAYMENT_SCORE to the sequential features list\n",
    "SEQUENTIAL_COLS_NUMERICAL = [\n",
    "    'INSTALLMENT_NO', 'INSTALLMENT_AMOUNT', 'DAYS_LATE', 'DAYS_BETWEEN_DUE_DATES',\n",
    "    'PAID_RATIO', 'DELTA_DAYS_LATE', 'PAYMENT_SCORE', 'COMPOSITE_RISK', \n",
    "    'RECENT_PAYMENT_SCORE', 'PAYMENT_SCORE_RANK', 'IS_UNPAID', 'CURRENT_EMI_BEHAVIOR_LABEL'\n",
    "]\n",
    "\n",
    "STATIC_COLS_NUMERICAL = ['TOTAL_INCOME', 'TOTAL_EXPENSE', 'LOAN_AMOUNT', 'AGE', 'CYCLE']\n",
    "STATIC_COLS_OHE = ['MARITAL_STATUS_NAME', 'STATE_NAME', 'LOAN_SCHEDULE_TYPE']\n",
    "STATIC_EMBEDDING_COLS = ['OCCUPATION_NAME_ENCODED', 'LOAN_PURPOSE_ENCODED']\n",
    "\n",
    "FLAG_ORDER = {'A': 4, 'B': 3, 'C': 2, 'D': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b88f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "‚úÖ Category mappings loaded successfully.\n",
      "‚úÖ Preprocessor and Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- ARTIFACT LOADING ---\n",
    "try:\n",
    "    preprocessor = joblib.load('preprocessor.pkl')\n",
    "    model = load_model('hybrid_lstm_model.h5', compile=False) \n",
    "    \n",
    "    # FIX #2: Load the category mappings for embedding features\n",
    "    try:\n",
    "        category_mappings = joblib.load('embedding_category_mappings.pkl')\n",
    "        print(\"‚úÖ Category mappings loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è WARNING: embedding_category_mappings.pkl not found. Creating dummy mappings.\")\n",
    "        print(\"   This may cause incorrect predictions. Please save mappings from training.\")\n",
    "        category_mappings = {'occupation': {}, 'purpose': {}}\n",
    "    \n",
    "    print(\"‚úÖ Preprocessor and Model loaded successfully.\")\n",
    "    \n",
    "    # Extract the full list of numerical features that went into the scaler\n",
    "    NUMERICAL_FEATURES_FINAL = SEQUENTIAL_COLS_NUMERICAL + STATIC_COLS_NUMERICAL\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå ERROR: Could not find required artifact: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbd627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HYBRID LSTM MODEL - BACK-TESTING PIPELINE\n",
      "================================================================================\n",
      "--- Loading LOS Data from PostgreSQL Table: Ashirvad ---\n",
      "--- Applying LOS Preprocessing (Matching Training Pipeline) ---\n",
      "   Imputed 0 missing values in TOTAL_EXPENSE with median: 1800.00\n",
      "   Imputed 0 missing values in TOTAL_INCOME with median: 21000.00\n",
      "   Imputed missing OCCUPATION_NAME with 'UNKNOWN'\n",
      "   Consolidated MARITAL_STATUS_NAME categories\n",
      "‚úÖ LOS Preprocessing Complete\n",
      "Memory reduction: 571.55 MB -> 134.06 MB (76.54% saved)\n",
      "‚úÖ Successfully fetched and preprocessed LOS data from DB. Shape: (1095563, 16)\n",
      "--- Loading 24 LMS Excel file(s) ---\n",
      "   ‚úÖ Loaded: test_001.xlsx\n",
      "   ‚úÖ Loaded: test_002.xlsx\n",
      "   ‚úÖ Loaded: test_003.xlsx\n",
      "   ‚úÖ Loaded: test_004.xlsx\n",
      "   ‚úÖ Loaded: test_005.xlsx\n",
      "   ‚úÖ Loaded: test_006.xlsx\n",
      "   ‚úÖ Loaded: test_007.xlsx\n",
      "   ‚úÖ Loaded: test_008.xlsx\n",
      "   ‚úÖ Loaded: test_009.xlsx\n",
      "   ‚úÖ Loaded: test_010.xlsx\n",
      "   ‚úÖ Loaded: test_011.xlsx\n",
      "   ‚úÖ Loaded: test_012.xlsx\n",
      "   ‚úÖ Loaded: test_013.xlsx\n",
      "   ‚úÖ Loaded: test_014.xlsx\n",
      "   ‚úÖ Loaded: test_015.xlsx\n",
      "   ‚úÖ Loaded: test_016.xlsx\n",
      "   ‚úÖ Loaded: test_017.xlsx\n",
      "   ‚úÖ Loaded: test_018.xlsx\n",
      "   ‚úÖ Loaded: test_019.xlsx\n",
      "   ‚úÖ Loaded: test_020.xlsx\n",
      "   ‚úÖ Loaded: test_021.xlsx\n",
      "   ‚úÖ Loaded: test_022.xlsx\n",
      "   ‚úÖ Loaded: test_023.xlsx\n",
      "   ‚úÖ Loaded: test_024.xlsx\n",
      "Memory reduction: 3003.00 MB -> 2355.65 MB (21.56% saved)\n",
      "--- Applying LMS Feature Engineering ---\n",
      "‚úÖ LMS Feature Engineering complete. Shape: (18855547, 25)\n",
      "\n",
      "--- Back-testing Mode: Trimming last 1 EMI(s) ---\n",
      "   LMS records after trimming: 17831866\n",
      "   Combined shape after merge: (17557669, 40)\n",
      "   Creating RECENT_PAYMENT_SCORE (rolling window feature)...\n",
      "   Applying custom encoding for CUSTOMER_FLAG, OCCUPATION_NAME, LOAN_PURPOSE...\n",
      "      OCCUPATION_NAME: 62 unique codes\n",
      "      LOAN_PURPOSE: 162 unique codes\n",
      "   One-hot encoding REPAYMENT_SCHEDULE_CAT...\n",
      "   Filtering to keep only necessary sequence (last MAX_SEQUENCE_LENGTH EMIs)...\n",
      "‚úÖ Data preparation complete. Final shape: (8067906, 35)\n",
      "   Unique loans: 1008504\n",
      "\n",
      "--- Step: Preprocessing and Prediction ---\n",
      "   Aligning OHE columns (5 expected from training)...\n",
      "   ‚ö†Ô∏è Adding 1 missing columns with zeros\n",
      "   Applying StandardScaler and OneHotEncoder...\n",
      "\n",
      "================================================================================\n",
      "‚ùå FATAL ERROR IN PREDICTION SCRIPT\n",
      "================================================================================\n",
      "Error Details: The feature names should match those that were passed during fit.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "\n",
      "Full Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\101032\\AppData\\Local\\Temp\\ipykernel_21624\\629015359.py\", line 469, in <module>\n",
      "    final_predictions_df, y_pred, y_actual = preprocess_and_predict(df_full_data)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\101032\\AppData\\Local\\Temp\\ipykernel_21624\\629015359.py\", line 337, in preprocess_and_predict\n",
      "    X_scaled_ohe = preprocessor.transform(df_for_transform)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 827, in transform\n",
      "    Xs = self._fit_transform(\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 681, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 940, in _transform_one\n",
      "    res = transformer.transform(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1006, in transform\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\base.py\", line 580, in _validate_data\n",
      "    self._check_feature_names(X, reset=reset)\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\sklearn\\base.py\", line 507, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# --- PART 1: DATA LOADING AND CORE FEATURE ENGINEERING (LOS & LMS) ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "def load_data_from_db():\n",
    "    \"\"\"Fetches LOS data from PostgreSQL using SQLAlchemy and applies ALL preprocessing from training.\"\"\"\n",
    "    conn_string = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "    try:\n",
    "        engine = sqlalchemy.create_engine(conn_string)\n",
    "        print(f\"--- Loading LOS Data from PostgreSQL Table: {LOS_TABLE_NAME} ---\")\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM \"{LOS_TABLE_NAME}\"\n",
    "        WHERE \"LOAN_DATE\" BETWEEN '{LOS_START_DATE}' AND '{LOS_END_DATE}';\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(query, con=engine)\n",
    "        \n",
    "        # ===== FIX #3: ADD COMPLETE LOS PREPROCESSING (MATCHING TRAINING) =====\n",
    "        \n",
    "        print(\"--- Applying LOS Preprocessing (Matching Training Pipeline) ---\")\n",
    "        \n",
    "        # 1. Convert PIN_CODE to string\n",
    "        if 'PIN_CODE' in df.columns:\n",
    "            df['PIN_CODE'] = df['PIN_CODE'].astype(str)\n",
    "        \n",
    "        # 2. Drop columns that were dropped in training\n",
    "        columns_to_drop = [\n",
    "            'CUSTOMER_ID', 'CUSTOMER_NAME', 'BRANCH_ID', 'TEMP_CUST_ID',\n",
    "            'PHONE1', 'PHONE2', 'HOUSE_NAME', 'LOCALITY', 'STREET',\n",
    "            'ALT_HOUSE_NAME', 'ALT_LOCALITY', 'ALT_STREET', 'CENTER_ID',\n",
    "            'ALT_PIN_CODE', 'MARITAL_STATUS', 'LOAN_STATUS',\n",
    "            'LOAN_STATUS_DESC', 'CLS_DT', 'CIBIL_ID', 'NPA_FLAG', \n",
    "            'NPA_FROM_DATE', 'NPA_TO_DATE', 'NPA_STATUS',\n",
    "            'OCCUPATION_ID', 'APPLICATION_ID', 'TENURE_in_months', \n",
    "            'emi_paid', 'loan_paid_percentage', 'NPA_STATUS_UPDATED', \n",
    "            'NPA_STATUS_UPDATED_1', 'NPA_STATUS_UPDATED_2', 'YEAR'\n",
    "        ]\n",
    "        df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "        \n",
    "        # 3. Drop CIBIL_SCORE and CUSTOMER_GRADING_SCORE\n",
    "        df.drop(columns=['CIBIL_SCORE', 'CUSTOMER_GRADING_SCORE'], inplace=True, errors='ignore')\n",
    "        \n",
    "        # 4. Calculate AGE\n",
    "        df['DATE_OF_BIRTH'] = pd.to_datetime(df['DATE_OF_BIRTH'], errors='coerce')\n",
    "        df['LOAN_DATE'] = pd.to_datetime(df['LOAN_DATE'], errors='coerce')\n",
    "        df['AGE'] = (df['LOAN_DATE'] - df['DATE_OF_BIRTH']).dt.days // 365.25\n",
    "        df.drop(columns=['DATE_OF_BIRTH'], inplace=True)\n",
    "        \n",
    "        # 5. Filter CUSTOMER_FLAG (remove 'X' records)\n",
    "        df = df[df['CUSTOMER_FLAG'] != 'X'].copy()\n",
    "        \n",
    "        # 6. Drop high-cardinality columns\n",
    "        df.drop(columns=['BRANCH_NAME', 'CENTER_NAME'], errors='ignore', inplace=True)\n",
    "        \n",
    "        # 7. Impute missing values in numerical columns\n",
    "        numerical_cols_to_impute = ['TOTAL_EXPENSE', 'TOTAL_INCOME', 'AGE']\n",
    "        for col in numerical_cols_to_impute:\n",
    "            if col in df.columns and df[col].isnull().sum() > 0:\n",
    "                median_val = df[col].median()\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "                print(f\"   Imputed {df[col].isnull().sum()} missing values in {col} with median: {median_val:.2f}\")\n",
    "        \n",
    "        # 8. Impute OCCUPATION_NAME with 'UNKNOWN'\n",
    "        if 'OCCUPATION_NAME' in df.columns and df['OCCUPATION_NAME'].isnull().sum() > 0:\n",
    "            df['OCCUPATION_NAME'] = df['OCCUPATION_NAME'].fillna('UNKNOWN')\n",
    "            print(f\"   Imputed missing OCCUPATION_NAME with 'UNKNOWN'\")\n",
    "        \n",
    "        # 9. Consolidate MARITAL_STATUS_NAME\n",
    "        if 'MARITAL_STATUS_NAME' in df.columns:\n",
    "            df['MARITAL_STATUS_NAME'] = df['MARITAL_STATUS_NAME'].replace(\n",
    "                ['UNMARRIED', 'SINGLE'], 'UNMARRIED/SINGLE')\n",
    "            print(f\"   Consolidated MARITAL_STATUS_NAME categories\")\n",
    "        \n",
    "        print(\"‚úÖ LOS Preprocessing Complete\")\n",
    "        \n",
    "        # ===== END OF LOS PREPROCESSING =====\n",
    "        \n",
    "        df = optimize_df_memory(df, verbose=True)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully fetched and preprocessed LOS data from DB. Shape: {df.shape}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection or query failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_and_engineer_lms_data():\n",
    "    \"\"\"Loads, cleans, and engineers sequential features for LMS data.\"\"\"\n",
    "    all_lms_dfs = []\n",
    "    lms_files = sorted(glob.glob(os.path.join(NEW_LMS_FOLDER_PATH, 'test_*.xlsx')))\n",
    "    \n",
    "    if not lms_files:\n",
    "        raise FileNotFoundError(f\"No Excel files found matching 'test_*.xlsx' in {NEW_LMS_FOLDER_PATH}.\")\n",
    "    \n",
    "    print(f\"--- Loading {len(lms_files)} LMS Excel file(s) ---\")\n",
    "    for file_name in lms_files:\n",
    "        try:\n",
    "            df = pd.read_excel(file_name)\n",
    "            all_lms_dfs.append(df)\n",
    "            print(f\"   ‚úÖ Loaded: {os.path.basename(file_name)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading {os.path.basename(file_name)}: {e}\")\n",
    "            \n",
    "    df_lms = pd.concat(all_lms_dfs, ignore_index=True)\n",
    "    df_lms = optimize_df_memory(df_lms, verbose=True)\n",
    "    \n",
    "    # --- LMS Core Cleaning ---\n",
    "    print(\"--- Applying LMS Feature Engineering ---\")\n",
    "    df_lms['DUE_DATE'] = pd.to_datetime(df_lms['DUE_DATE'], errors='coerce')\n",
    "    df_lms['PAID_DT'] = pd.to_datetime(df_lms['PAID_DT'], errors='coerce')\n",
    "    df_lms.loc[df_lms['STATUS'] == 1, 'PAID_DT'] = pd.NaT \n",
    "    df_lms = df_lms.sort_values(by=['LOAN_ID', 'INSTALLMENT_NO']).reset_index(drop=True)\n",
    "    \n",
    "    # --- LMS Feature Engineering ---\n",
    "    df_lms['DAYS_LATE'] = (df_lms['PAID_DT'] - df_lms['DUE_DATE']).dt.days\n",
    "    df_lms['DAYS_BETWEEN_DUE_DATES'] = df_lms.groupby('LOAN_ID')['DUE_DATE'].diff().dt.days\n",
    "    \n",
    "    # Repayment Schedule Category\n",
    "    df_lms['REPAYMENT_SCHEDULE_CAT'] = np.select(\n",
    "        [\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'].isnull(), \n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'].isin([28, 29, 30, 31]), \n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'] == 7,\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'] == 14,\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'] == 56\n",
    "        ],\n",
    "        ['Initial', 'Monthly', 'Weekly', 'Bi-Weekly', 'Bi-Monthly'], \n",
    "        default='Other'\n",
    "    )\n",
    "    \n",
    "    # Loan Schedule Type\n",
    "    has_weekly = df_lms.groupby('LOAN_ID')['REPAYMENT_SCHEDULE_CAT'].transform(\n",
    "        lambda x: 'Weekly' in x.unique())\n",
    "    has_monthly = df_lms.groupby('LOAN_ID')['REPAYMENT_SCHEDULE_CAT'].transform(\n",
    "        lambda x: 'Monthly' in x.unique())\n",
    "    mode_schedule = df_lms.groupby('LOAN_ID')['REPAYMENT_SCHEDULE_CAT'].transform(\n",
    "        lambda x: x.mode()[0] if not x.mode().empty else 'Initial')\n",
    "    df_lms['LOAN_SCHEDULE_TYPE'] = np.where((has_weekly) & (has_monthly), 'Hybrid', mode_schedule)\n",
    "    \n",
    "    # Payment behavior features\n",
    "    df_lms['IS_UNPAID'] = np.where(df_lms['STATUS'] == 1, 1, 0)\n",
    "    df_lms['DAYS_LATE'] = df_lms['DAYS_LATE'].fillna(0)\n",
    "    df_lms['PAID_RATIO'] = df_lms['PAID_AMOUNT'] / df_lms['INSTALLMENT_AMOUNT']\n",
    "    df_lms['PAID_RATIO'] = df_lms['PAID_RATIO'].clip(upper=1.0)\n",
    "    df_lms['DELTA_DAYS_LATE'] = df_lms.groupby('LOAN_ID')['DAYS_LATE'].diff().fillna(0)\n",
    "    df_lms.loc[df_lms['IS_UNPAID'] == 1, 'DELTA_DAYS_LATE'] = 0 \n",
    "    \n",
    "    # Current EMI Behavior Label\n",
    "    conditions_behavior = [\n",
    "        (df_lms['STATUS'] == 1) | \n",
    "        ((df_lms['STATUS'] == 0) & (df_lms['DAYS_LATE'] > GRACE_PERIOD_DAYS)) | \n",
    "        ((df_lms['STATUS'] == 2) & (\n",
    "            (df_lms['DAYS_LATE'] > GRACE_PERIOD_DAYS) | \n",
    "            (df_lms['PAID_AMOUNT'] / df_lms['INSTALLMENT_AMOUNT'] < PAID_PERCENTAGE_THRESHOLD)\n",
    "        )),\n",
    "        (df_lms['STATUS'] == 0) & (df_lms['DAYS_LATE'] <= GRACE_PERIOD_DAYS),\n",
    "        (df_lms['STATUS'] == 2) & (df_lms['DAYS_LATE'] <= GRACE_PERIOD_DAYS) & \n",
    "        (df_lms['PAID_AMOUNT'] / df_lms['INSTALLMENT_AMOUNT'] >= PAID_PERCENTAGE_THRESHOLD)\n",
    "    ]\n",
    "    choices_behavior = [1, 0, 0]\n",
    "    df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] = np.select(\n",
    "        conditions_behavior, choices_behavior, default=-1).astype(np.int8)\n",
    "    \n",
    "    # Composite Risk and Payment Score\n",
    "    df_lms['REMAINING_EMI_RATIO'] = 1 - df_lms[\"PAID_RATIO\"]\n",
    "    df_lms['COMPOSITE_RISK'] = df_lms['DAYS_LATE'] + (df_lms['REMAINING_EMI_RATIO'] * 10)\n",
    "    \n",
    "    conditions_score = [\n",
    "        df_lms['IS_UNPAID'] == 1,\n",
    "        (df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] == 1) & (df_lms['IS_UNPAID'] == 0),\n",
    "        (df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] == 0) & (df_lms['DAYS_LATE'] > 0) & \n",
    "        (df_lms['DAYS_LATE'] <= GRACE_PERIOD_DAYS) & (df_lms['IS_UNPAID'] == 0),\n",
    "        (df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] == 0) & (df_lms['DAYS_LATE'] <= 0) & \n",
    "        (df_lms['IS_UNPAID'] == 0)\n",
    "    ]\n",
    "    \n",
    "    choices_score = [\n",
    "        -100,\n",
    "        np.maximum(0.0, np.minimum(0.30, 0.30 - (df_lms['COMPOSITE_RISK'] * 0.03))),\n",
    "        1.0 / (1 + df_lms['COMPOSITE_RISK']),\n",
    "        1.5 + (np.abs(df_lms['DAYS_LATE']) / 10)\n",
    "    ]\n",
    "    df_lms['PAYMENT_SCORE'] = np.select(conditions_score, choices_score, default=0).astype(np.float32)\n",
    "    \n",
    "    rank_choices = [4, 3, 2, 1]\n",
    "    df_lms['PAYMENT_SCORE_RANK'] = np.select(conditions_score, rank_choices).astype(np.int8)\n",
    "\n",
    "    # --- VALIDATION MODE: DEFINE TARGET (EMI N) ---\n",
    "    actual_targets = df_lms.groupby('LOAN_ID').tail(TARGET_BACKTEST_EMI).copy()\n",
    "    actual_targets = actual_targets[['LOAN_ID', 'CURRENT_EMI_BEHAVIOR_LABEL']].rename(\n",
    "        columns={'CURRENT_EMI_BEHAVIOR_LABEL': 'ACTUAL_NEXT_EMI_ISSUE'}\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ LMS Feature Engineering complete. Shape: {df_lms.shape}\")\n",
    "    return df_lms, actual_targets\n",
    "\n",
    "def merge_and_finalize_data():\n",
    "    \"\"\"Merges LOS and LMS, applies final feature engineering, and performs back-test trimming.\"\"\"\n",
    "    \n",
    "    # 1. Load Data\n",
    "    df_los = load_data_from_db()\n",
    "    df_lms_full, df_actual_targets = load_and_engineer_lms_data()\n",
    "    \n",
    "    # 2. VALIDATION MODE: Trim the last EMI (EMI N) from the sequence\n",
    "    print(f\"\\n--- Back-testing Mode: Trimming last {TARGET_BACKTEST_EMI} EMI(s) ---\")\n",
    "    indices_to_drop = df_lms_full.groupby('LOAN_ID').tail(TARGET_BACKTEST_EMI).index\n",
    "    df_lms = df_lms_full.drop(indices_to_drop).reset_index(drop=True)\n",
    "    print(f\"   LMS records after trimming: {df_lms.shape[0]}\")\n",
    "    \n",
    "    # 3. Rename and Merge LOS\n",
    "    RENAME_MAP_LOS = {\n",
    "        'LOAN_AMOUNT': 'LOAN_AMOUNT_STATIC', \n",
    "        'TENURE': 'TENURE_STATIC', \n",
    "        'INTEREST_RATE': 'INTEREST_RATE_STATIC'\n",
    "    }\n",
    "    df_los.rename(columns=RENAME_MAP_LOS, inplace=True)\n",
    "    df_combined = pd.merge(df_los, df_lms, on='LOAN_ID', how='inner', suffixes=('_static', '_lms'))\n",
    "    print(f\"   Combined shape after merge: {df_combined.shape}\")\n",
    "    \n",
    "    # 4. Final Cleaning and Rename\n",
    "    df_combined.rename(columns={\n",
    "        \"LOAN_DATE_static\": \"LOAN_DATE\", \n",
    "        \"LOAN_AMOUNT_STATIC\": \"LOAN_AMOUNT\", \n",
    "        \"TENURE_STATIC\": \"TENURE\", \n",
    "        \"INTEREST_RATE_STATIC\": \"INTEREST_RATE\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    cols_to_drop = [\n",
    "        c for c in ['LOAN_DATE_lms', 'LOAN_AMOUNT_lms', 'TENURE_lms', 'INTEREST_RATE_lms', \n",
    "                    'DISBURSED_AMOUNT', 'PIN_CODE', 'IS_DAYS_LATE_MISSING', 'STATUS', \n",
    "                    'PAID_DT', 'PAID_AMOUNT', 'REMAINING_EMI_RATIO', 'TENURE', 'INTEREST_RATE'] \n",
    "        if c in df_combined.columns\n",
    "    ]\n",
    "    df_combined.drop(columns=cols_to_drop, errors='ignore', inplace=True)\n",
    "    df_combined[\"DAYS_BETWEEN_DUE_DATES\"].fillna(0, inplace=True)\n",
    "\n",
    "    # 5. Rolling Feature (RECENT_PAYMENT_SCORE) - Shifted, then imputed\n",
    "    print(\"   Creating RECENT_PAYMENT_SCORE (rolling window feature)...\")\n",
    "    df_combined['RECENT_PAYMENT_SCORE'] = df_combined.groupby('LOAN_ID')['PAYMENT_SCORE'].rolling(\n",
    "        window=ROLLING_WINDOW_SIZE, min_periods=1\n",
    "    ).mean().reset_index(level=0, drop=True).astype(np.float32)\n",
    "    df_combined['RECENT_PAYMENT_SCORE'] = df_combined.groupby('LOAN_ID')['RECENT_PAYMENT_SCORE'].shift(1)\n",
    "    overall_mean_score = df_combined['PAYMENT_SCORE'].mean()\n",
    "    df_combined['RECENT_PAYMENT_SCORE'].fillna(overall_mean_score, inplace=True)\n",
    "\n",
    "    # 6. Custom Encoding\n",
    "    print(\"   Applying custom encoding for CUSTOMER_FLAG, OCCUPATION_NAME, LOAN_PURPOSE...\")\n",
    "    df_combined['CUSTOMER_FLAG_ENCODED'] = df_combined['CUSTOMER_FLAG'].astype(str).map(FLAG_ORDER).fillna(0).astype(np.int8)\n",
    "    \n",
    "    # FIX #4: Use saved category mappings for consistent encoding\n",
    "    if category_mappings['occupation'] and category_mappings['purpose']:\n",
    "        # Create reverse mappings (category -> code)\n",
    "        occupation_to_code = {v: k+1 for k, v in category_mappings['occupation'].items()}\n",
    "        purpose_to_code = {v: k+1 for k, v in category_mappings['purpose'].items()}\n",
    "        \n",
    "        # Apply encoding with training mappings, use 0 for unknown categories\n",
    "        df_combined['OCCUPATION_NAME_ENCODED'] = df_combined['OCCUPATION_NAME'].astype(str).map(\n",
    "            occupation_to_code).fillna(0).astype(np.int16)\n",
    "        df_combined['LOAN_PURPOSE_ENCODED'] = df_combined['LOAN_PURPOSE'].astype(str).map(\n",
    "            purpose_to_code).fillna(0).astype(np.int16)\n",
    "        \n",
    "        print(f\"      OCCUPATION_NAME: {df_combined['OCCUPATION_NAME_ENCODED'].nunique()} unique codes\")\n",
    "        print(f\"      LOAN_PURPOSE: {df_combined['LOAN_PURPOSE_ENCODED'].nunique()} unique codes\")\n",
    "    else:\n",
    "        # Fallback to categorical codes (may be inconsistent with training)\n",
    "        print(\"   ‚ö†Ô∏è WARNING: Using fallback encoding (may be inconsistent with training)\")\n",
    "        df_combined['OCCUPATION_NAME_ENCODED'] = df_combined['OCCUPATION_NAME'].astype(\n",
    "            'category').cat.codes + 1\n",
    "        df_combined['LOAN_PURPOSE_ENCODED'] = df_combined['LOAN_PURPOSE'].astype(\n",
    "            'category').cat.codes + 1\n",
    "    \n",
    "    df_combined.drop(columns=['CUSTOMER_FLAG', 'OCCUPATION_NAME', 'LOAN_PURPOSE', 'LOAN_DATE'], \n",
    "                     errors='ignore', inplace=True)\n",
    "    \n",
    "    # 7. OHE Sequential Categorical Feature\n",
    "    print(\"   One-hot encoding REPAYMENT_SCHEDULE_CAT...\")\n",
    "    df_combined = pd.get_dummies(df_combined, columns=['REPAYMENT_SCHEDULE_CAT'], prefix='REPAYMENT_CAT')\n",
    "\n",
    "    # 8. Filter for the latest sequence for prediction (up to EMI N-1)\n",
    "    print(\"   Filtering to keep only necessary sequence (last MAX_SEQUENCE_LENGTH EMIs)...\")\n",
    "    df_combined['SEQUENCE_COUNT'] = df_combined.groupby('LOAN_ID').cumcount() + 1\n",
    "    df_combined['REVERSE_SEQUENCE_COUNT'] = df_combined.groupby('LOAN_ID')['SEQUENCE_COUNT'].transform(\n",
    "        'max') - df_combined['SEQUENCE_COUNT']\n",
    "    df_combined_filtered = df_combined[df_combined['REVERSE_SEQUENCE_COUNT'] < MAX_SEQUENCE_LENGTH].copy()\n",
    "    df_combined_filtered.drop(columns=['SEQUENCE_COUNT', 'REVERSE_SEQUENCE_COUNT'], inplace=True)\n",
    "    \n",
    "    # 9. Merge Actual Target for Validation\n",
    "    df_combined_filtered = pd.merge(df_combined_filtered, df_actual_targets, on='LOAN_ID', how='left')\n",
    "    df_combined_filtered['ACTUAL_NEXT_EMI_ISSUE'] = df_combined_filtered.groupby(\n",
    "        'LOAN_ID')['ACTUAL_NEXT_EMI_ISSUE'].transform('max')\n",
    "    df_combined_filtered.dropna(subset=['ACTUAL_NEXT_EMI_ISSUE'], inplace=True)\n",
    "\n",
    "    print(f\"‚úÖ Data preparation complete. Final shape: {df_combined_filtered.shape}\")\n",
    "    print(f\"   Unique loans: {df_combined_filtered['LOAN_ID'].nunique()}\")\n",
    "    \n",
    "    return df_combined_filtered\n",
    "\n",
    "def preprocess_and_predict(df_full):\n",
    "    \"\"\"Applies preprocessor, reshapes sequences, and runs inference.\"\"\"\n",
    "    \n",
    "    print(\"\\n--- Step: Preprocessing and Prediction ---\")\n",
    "    \n",
    "    # FIX #5: Alignment of OHE Columns with Training\n",
    "    training_ohe_cols = [c for c in preprocessor.feature_names_in_ if c.startswith('REPAYMENT_CAT_')]\n",
    "    print(f\"   Aligning OHE columns ({len(training_ohe_cols)} expected from training)...\")\n",
    "    for col in training_ohe_cols:\n",
    "        if col not in df_full.columns:\n",
    "            df_full[col] = 0\n",
    "            print(f\"      Added missing column: {col}\")\n",
    "    \n",
    "    # FIX #6: Get expected features from preprocessor and ensure all are present\n",
    "    expected_features = list(preprocessor.feature_names_in_)\n",
    "    \n",
    "    # Add CUSTOMER_FLAG_ENCODED to static numerical if not present\n",
    "    if 'CUSTOMER_FLAG_ENCODED' not in STATIC_COLS_NUMERICAL:\n",
    "        STATIC_COLS_NUMERICAL_FINAL = STATIC_COLS_NUMERICAL + ['CUSTOMER_FLAG_ENCODED']\n",
    "    else:\n",
    "        STATIC_COLS_NUMERICAL_FINAL = STATIC_COLS_NUMERICAL\n",
    "    \n",
    "    # Define the columns that should be transformed (numerical + OHE categorical)\n",
    "    NUMERICAL_FEATURES_WITH_OHE = SEQUENTIAL_COLS_NUMERICAL + STATIC_COLS_NUMERICAL_FINAL + training_ohe_cols\n",
    "    ALL_TRANSFORM_COLS = NUMERICAL_FEATURES_WITH_OHE + STATIC_COLS_OHE\n",
    "    \n",
    "    # Check for missing columns and add them with zeros\n",
    "    missing_cols = set(expected_features) - set(df_full.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"   ‚ö†Ô∏è Adding {len(missing_cols)} missing columns with zeros\")\n",
    "        for col in missing_cols:\n",
    "            df_full[col] = 0\n",
    "    \n",
    "    # Ensure column order matches training\n",
    "    df_for_transform = df_full[expected_features].copy()\n",
    "    \n",
    "    # FIX #7: Apply ColumnTransformer\n",
    "    print(\"   Applying StandardScaler and OneHotEncoder...\")\n",
    "    X_scaled_ohe = preprocessor.transform(df_for_transform)\n",
    "    ALL_FINAL_COLS = list(preprocessor.get_feature_names_out())\n",
    "    \n",
    "    # Reconstruct DataFrame\n",
    "    X_predict_df = pd.DataFrame(X_scaled_ohe, columns=ALL_FINAL_COLS, index=df_full.index)\n",
    "    \n",
    "    # Add back non-transformed columns\n",
    "    X_predict_df['LOAN_ID'] = df_full['LOAN_ID'].values\n",
    "    X_predict_df['ACTUAL_NEXT_EMI_ISSUE'] = df_full['ACTUAL_NEXT_EMI_ISSUE'].values\n",
    "    for col in STATIC_EMBEDDING_COLS:\n",
    "        X_predict_df[col] = df_full[col].values\n",
    "\n",
    "    # Define Final Input Feature Sets with correct prefixes\n",
    "    LSTM_INPUT_COLS_FINAL = [f\"num__{c}\" for c in SEQUENTIAL_COLS_NUMERICAL] + \\\n",
    "                            [f\"num__{c}\" for c in training_ohe_cols]\n",
    "    \n",
    "    STATIC_DENSE_COLS_FINAL = [f\"num__{c}\" for c in STATIC_COLS_NUMERICAL_FINAL] + \\\n",
    "                              [c for c in ALL_FINAL_COLS if c.startswith('cat__')]\n",
    "\n",
    "    print(f\"   LSTM input features: {len(LSTM_INPUT_COLS_FINAL)}\")\n",
    "    print(f\"   Static dense features: {len(STATIC_DENSE_COLS_FINAL)}\")\n",
    "    print(f\"   Embedding features: {len(STATIC_EMBEDDING_COLS)}\")\n",
    "\n",
    "    # Sequence Reshaping Function\n",
    "    def reshape_for_prediction(X_df, lstm_cols, static_dense_cols, embedding_cols, max_len):\n",
    "        grouped = X_df.groupby('LOAN_ID')\n",
    "        loan_ids = list(grouped.groups.keys())\n",
    "        \n",
    "        X_lstm = np.zeros((len(loan_ids), max_len, len(lstm_cols)), dtype=np.float32)\n",
    "        X_static_dense = np.zeros((len(loan_ids), len(static_dense_cols)), dtype=np.float32)\n",
    "        X_static_embed = np.zeros((len(loan_ids), len(embedding_cols)), dtype=np.int16)\n",
    "        y_actual = np.zeros((len(loan_ids),), dtype=np.int8)\n",
    "        \n",
    "        for i, loan_id in enumerate(loan_ids):\n",
    "            loan_data = grouped.get_group(loan_id)\n",
    "            sequence = loan_data[lstm_cols].values\n",
    "            \n",
    "            # Pad or truncate sequence\n",
    "            if len(sequence) >= max_len:\n",
    "                X_lstm[i, :, :] = sequence[-max_len:]\n",
    "            else:\n",
    "                X_lstm[i, -len(sequence):, :] = sequence\n",
    "\n",
    "            # Get static features from last record\n",
    "            last_record = loan_data.iloc[-1]\n",
    "            X_static_dense[i, :] = last_record[static_dense_cols].values\n",
    "            X_static_embed[i, :] = last_record[embedding_cols].values.astype(np.int16)\n",
    "            y_actual[i] = last_record['ACTUAL_NEXT_EMI_ISSUE'].astype(np.int8)\n",
    "\n",
    "        return X_lstm, X_static_dense, X_static_embed, loan_ids, y_actual\n",
    "\n",
    "    # Execute reshaping\n",
    "    print(\"   Reshaping sequences for model input...\")\n",
    "    X_predict_lstm, X_predict_static_dense, X_predict_static_embed, prediction_loan_ids, y_actual = \\\n",
    "        reshape_for_prediction(X_predict_df, LSTM_INPUT_COLS_FINAL, STATIC_DENSE_COLS_FINAL, \n",
    "                              STATIC_EMBEDDING_COLS, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    print(f\"   Reshaped data shapes:\")\n",
    "    print(f\"      LSTM: {X_predict_lstm.shape}\")\n",
    "    print(f\"      Static Dense: {X_predict_static_dense.shape}\")\n",
    "    print(f\"      Embedding: {X_predict_static_embed.shape}\")\n",
    "\n",
    "    # Model Inference\n",
    "    print(\"\\n   Running model inference...\")\n",
    "    prediction_inputs = {\n",
    "        'lstm_input': X_predict_lstm, \n",
    "        'static_dense_input': X_predict_static_dense, \n",
    "        'embedding_input': X_predict_static_embed\n",
    "    }\n",
    "\n",
    "    y_pred_proba = model.predict(prediction_inputs, verbose=0)[:, 0]\n",
    "    y_pred_class = (y_pred_proba > 0.5).astype(np.int8)\n",
    "\n",
    "    # Inverse Transform Installment Number\n",
    "    print(\"   Extracting and inverse-transforming installment numbers...\")\n",
    "    INSTALLMENT_NO_COL_NAME_SCALED = 'num__INSTALLMENT_NO'\n",
    "    CURRENT_EMI_BEHAVIOR_LABEL_COL_NAME_SCALED = 'num__CURRENT_EMI_BEHAVIOR_LABEL'\n",
    "    \n",
    "    try:\n",
    "        INSTALLMENT_NO_IDX = LSTM_INPUT_COLS_FINAL.index(INSTALLMENT_NO_COL_NAME_SCALED)\n",
    "        CURRENT_EMI_BEHAVIOR_LABEL_IDX = LSTM_INPUT_COLS_FINAL.index(CURRENT_EMI_BEHAVIOR_LABEL_COL_NAME_SCALED)\n",
    "    except ValueError as e:\n",
    "        print(f\"   ‚ùå ERROR: Required column not found in LSTM features: {e}\")\n",
    "        raise\n",
    "    \n",
    "    LAST_TIME_STEP_IDX = X_predict_lstm.shape[1] - 1\n",
    "    \n",
    "    # Get scaler parameters for inverse transform\n",
    "    scaler = preprocessor.named_transformers_['num']\n",
    "    try:\n",
    "        original_feature_index = list(preprocessor.feature_names_in_).index('INSTALLMENT_NO')\n",
    "    except ValueError:\n",
    "        print(\"   ‚ö†Ô∏è WARNING: INSTALLMENT_NO not found in preprocessor features. Using index 0.\")\n",
    "        original_feature_index = 0\n",
    "    \n",
    "    install_no_mean = scaler.mean_[original_feature_index]\n",
    "    install_no_scale = scaler.scale_[original_feature_index]\n",
    "\n",
    "    # Inverse transform\n",
    "    scaled_install_no = X_predict_lstm[:, LAST_TIME_STEP_IDX, INSTALLMENT_NO_IDX]\n",
    "    unscaled_install_no = (scaled_install_no * install_no_scale) + install_no_mean\n",
    "    install_no_final_unscaled = np.round(unscaled_install_no + 1).astype(np.int16)\n",
    "\n",
    "    # Extract Current Behavior\n",
    "    behavior_label_final = X_predict_lstm[:, LAST_TIME_STEP_IDX, CURRENT_EMI_BEHAVIOR_LABEL_IDX].astype(np.int8)\n",
    "\n",
    "    # Final Output DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'LOAN_ID': prediction_loan_ids,\n",
    "        'LAST_EMI_USED_IN_PREDICTION': np.round(unscaled_install_no).astype(np.int16),\n",
    "        'PREDICTED_EMI_NO': install_no_final_unscaled,\n",
    "        'LAST_EMI_BEHAVIOR_USED_DESC': [TWO_CLASS_STATUS_MAP.get(lbl, 'Unknown') for lbl in behavior_label_final],\n",
    "        'PROBABILITY_OF_EMI_ISSUE': y_pred_proba.astype(np.float32),\n",
    "        'PREDICTED_EMI_ISSUE': y_pred_class,\n",
    "        'PREDICTED_EMI_STATUS': [TWO_CLASS_STATUS_MAP.get(pred, 'Unknown') for pred in y_pred_class],\n",
    "        'ACTUAL_EMI_ISSUE': y_actual,\n",
    "        'ACTUAL_EMI_STATUS': [TWO_CLASS_STATUS_MAP.get(actual, 'Unknown') for actual in y_actual]\n",
    "    })\n",
    "    \n",
    "    return results_df, y_pred_class, y_actual\n",
    "\n",
    "# --- MAIN EXECUTION BLOCK ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"=\"*80)\n",
    "        print(\"HYBRID LSTM MODEL - BACK-TESTING PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load, Merge, and Engineer Data\n",
    "        df_full_data = merge_and_finalize_data()\n",
    "\n",
    "        # Preprocess, Reshape, and Predict\n",
    "        final_predictions_df, y_pred, y_actual = preprocess_and_predict(df_full_data)\n",
    "\n",
    "        # --- Performance Calculation ---\n",
    "        print(\"\\n--- Calculating Performance Metrics ---\")\n",
    "        accuracy = accuracy_score(y_actual, y_pred)\n",
    "        cm = confusion_matrix(y_actual, y_pred)\n",
    "        report = classification_report(y_actual, y_pred, \n",
    "                                      target_names=['Paid (0)', 'Not Paid (1)'], \n",
    "                                      output_dict=True)\n",
    "\n",
    "        # Export Results\n",
    "        OUTPUT_FILE = 'Oct2025_Backtest_Predictions_Results.xlsx'\n",
    "        final_predictions_df.to_excel(OUTPUT_FILE, index=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ BACK-TESTING COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total Loans Evaluated: {final_predictions_df.shape[0]}\")\n",
    "        print(f\"Output saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "        print(\"\\n### üìà MODEL PERFORMANCE ON BACK-TEST DATA ###\")\n",
    "        print(f\"Overall Accuracy: {accuracy:.4f}\\n\")\n",
    "        \n",
    "        print(\"Confusion Matrix:\")\n",
    "        cm_df = pd.DataFrame(cm, \n",
    "                            index=['Actual Paid (0)', 'Actual Not Paid (1)'], \n",
    "                            columns=['Predicted Paid (0)', 'Predicted Not Paid (1)'])\n",
    "        print(cm_df)\n",
    "        \n",
    "        print(\"\\n Classification Report (Key Metrics):\")\n",
    "        print(f\"Precision (Not Paid): {report['Not Paid (1)']['precision']:.4f}\")\n",
    "        print(f\"Recall (Not Paid): {report['Not Paid (1)']['recall']:.4f}\")\n",
    "        print(f\"F1-Score (Not Paid): {report['Not Paid (1)']['f1-score']:.4f}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"\\nSample Results (First 10 Predictions):\")\n",
    "        print(\"=\"*80)\n",
    "        display_cols = ['LOAN_ID', 'PREDICTED_EMI_NO', 'PROBABILITY_OF_EMI_ISSUE', \n",
    "                       'PREDICTED_EMI_STATUS', 'ACTUAL_EMI_STATUS']\n",
    "        print(final_predictions_df[display_cols].head(10).to_string(index=False))\n",
    "        \n",
    "        print(\"\\n‚úÖ All operations completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ùå FATAL ERROR IN PREDICTION SCRIPT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Error Details: {e}\")\n",
    "        import traceback\n",
    "        print(\"\\nFull Traceback:\")\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62107b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_local_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
