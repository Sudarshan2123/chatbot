{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd5f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HYBRID LSTM MODEL - PRODUCTION PREDICTION PIPELINE\n",
    "# Purpose: Back-testing and Real-time Prediction for EMI Payment Default Risk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import glob\n",
    "import os\n",
    "import sqlalchemy\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550242b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ===================================================================================\n",
    "\n",
    "# Database Configuration\n",
    "DB_CONFIG = {\n",
    "    'host': '10.192.5.43',\n",
    "    'port': 5432,\n",
    "    'database': 'postgres',\n",
    "    'user': 'ml_db',\n",
    "    'password': 'pass%401234'\n",
    "}\n",
    "\n",
    "# Data Configuration\n",
    "LOS_TABLE_NAME = \"Ashirvad\"\n",
    "LOS_START_DATE = '2024-01-01'\n",
    "LOS_END_DATE = '2024-12-31'\n",
    "NEW_LMS_FOLDER_PATH = \"TestData_Oct2025prediction\"\n",
    "\n",
    "# Model Configuration\n",
    "MAX_SEQUENCE_LENGTH = 8\n",
    "ROLLING_WINDOW_SIZE = 3\n",
    "GRACE_PERIOD_DAYS = 2\n",
    "PAID_PERCENTAGE_THRESHOLD = 0.90\n",
    "\n",
    "# Validation Configuration\n",
    "TARGET_BACKTEST_EMI = 1  # Predict EMI N using data up to EMI N-1\n",
    "\n",
    "# Status Mapping\n",
    "TWO_CLASS_STATUS_MAP = {0: 'Paid', 1: 'Not Paid'}\n",
    "FLAG_ORDER = {'A': 4, 'B': 3, 'C': 2, 'D': 1}\n",
    "\n",
    "# Feature Definitions (MUST MATCH TRAINING EXACTLY)\n",
    "SEQUENTIAL_COLS_NUMERICAL = [\n",
    "    'INSTALLMENT_NO', 'INSTALLMENT_AMOUNT', 'DAYS_LATE', 'DAYS_BETWEEN_DUE_DATES',\n",
    "    'PAID_RATIO', 'DELTA_DAYS_LATE', 'PAYMENT_SCORE', 'COMPOSITE_RISK',\n",
    "    'RECENT_PAYMENT_SCORE', 'PAYMENT_SCORE_RANK', 'IS_UNPAID', 'CURRENT_EMI_BEHAVIOR_LABEL'\n",
    "]\n",
    "\n",
    "STATIC_COLS_NUMERICAL = ['TOTAL_INCOME', 'TOTAL_EXPENSE', 'LOAN_AMOUNT', 'AGE', 'CYCLE']\n",
    "STATIC_COLS_OHE = ['MARITAL_STATUS_NAME', 'STATE_NAME', 'LOAN_SCHEDULE_TYPE']\n",
    "STATIC_EMBEDDING_COLS = ['OCCUPATION_NAME_ENCODED', 'LOAN_PURPOSE_ENCODED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f96740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artifacts():\n",
    "    \"\"\"\n",
    "    Load all required model artifacts. \n",
    "    Strict Version: Will raise a FileNotFoundError and stop execution if ANY file is missing.\n",
    "    \"\"\"\n",
    "    # 1. Define required files\n",
    "    artifact_paths = {\n",
    "        'preprocessor': 'preprocessor.pkl',\n",
    "        'model': 'hybrid_lstm_model.h5',\n",
    "        'mappings': 'embedding_category_mappings.pkl'\n",
    "    }\n",
    "\n",
    "    # 2. Pre-check existence for clear reporting\n",
    "    missing_files = [path for path in artifact_paths.values() if not os.path.exists(path)]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(\"\\n\" + \"!\" * 60)\n",
    "        print(\"‚ùå CRITICAL ERROR: MISSION-CRITICAL ARTIFACTS MISSING\")\n",
    "        print(f\"The following required files were not found: {missing_files}\")\n",
    "        print(\"Pipeline execution halted to prevent incorrect predictions.\")\n",
    "        print(\"!\" * 60 + \"\\n\")\n",
    "        # Explicitly raise error to stop the script\n",
    "        raise FileNotFoundError(f\"Missing artifacts: {missing_files}\")\n",
    "\n",
    "    # 3. Load artifacts strictly\n",
    "    try:\n",
    "        preprocessor = joblib.load(artifact_paths['preprocessor'])\n",
    "        model = load_model(artifact_paths['model'], compile=False)\n",
    "        category_mappings = joblib.load(artifact_paths['mappings'])\n",
    "        \n",
    "        print(\"‚úÖ Success: Preprocessor, Model, and Mappings loaded.\")\n",
    "        return preprocessor, model, category_mappings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: Artifacts present but failed to load. Check for corruption or version mismatch: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf34c929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "‚úÖ Success: Preprocessor, Model, and Mappings loaded.\n",
      "--- Loading LOS Data from PostgreSQL Table: Ashirvad ---\n",
      "‚úÖ LOS data loaded. Shape: (1111834, 52)\n",
      "--- Applying LOS Preprocessing (Matching Training) ---\n",
      "   Imputed TOTAL_INCOME with value: 20000.00\n",
      "   Imputed TOTAL_EXPENSE with value: 1760.00\n",
      "‚úÖ LOS Preprocessing Complete. Shape: (1095563, 16)\n",
      "--- Loading 24 LMS Excel file(s) ---\n",
      "   ‚úÖ Loaded: test_001.xlsx\n",
      "   ‚úÖ Loaded: test_002.xlsx\n",
      "   ‚úÖ Loaded: test_003.xlsx\n",
      "   ‚úÖ Loaded: test_004.xlsx\n",
      "   ‚úÖ Loaded: test_005.xlsx\n",
      "   ‚úÖ Loaded: test_006.xlsx\n",
      "   ‚úÖ Loaded: test_007.xlsx\n",
      "   ‚úÖ Loaded: test_008.xlsx\n",
      "   ‚úÖ Loaded: test_009.xlsx\n",
      "   ‚úÖ Loaded: test_010.xlsx\n",
      "   ‚úÖ Loaded: test_011.xlsx\n",
      "   ‚úÖ Loaded: test_012.xlsx\n",
      "   ‚úÖ Loaded: test_013.xlsx\n",
      "   ‚úÖ Loaded: test_014.xlsx\n",
      "   ‚úÖ Loaded: test_015.xlsx\n",
      "   ‚úÖ Loaded: test_016.xlsx\n",
      "   ‚úÖ Loaded: test_017.xlsx\n",
      "   ‚úÖ Loaded: test_018.xlsx\n",
      "   ‚úÖ Loaded: test_019.xlsx\n",
      "   ‚úÖ Loaded: test_020.xlsx\n",
      "   ‚úÖ Loaded: test_021.xlsx\n",
      "   ‚úÖ Loaded: test_022.xlsx\n",
      "   ‚úÖ Loaded: test_023.xlsx\n",
      "   ‚úÖ Loaded: test_024.xlsx\n",
      "--- Applying LMS Feature Engineering ---\n",
      "‚úÖ LMS Feature Engineering Complete. Shape: (18855547, 26)\n",
      "\n",
      "--- Back-testing Mode: Trimming last 1 EMI(s) ---\n",
      "   LMS records after trimming: 17831866\n",
      "   Combined shape after merge: (17557669, 41)\n",
      "   Creating RECENT_PAYMENT_SCORE (rolling window feature)...\n",
      "   Applying custom encoding for CUSTOMER_FLAG, OCCUPATION_NAME, LOAN_PURPOSE...\n",
      "      OCCUPATION_NAME: 62 unique codes\n",
      "      LOAN_PURPOSE: 162 unique codes\n",
      "   One-hot encoding REPAYMENT_SCHEDULE_CAT...\n",
      "   Filtering to keep only necessary sequence (last MAX_SEQUENCE_LENGTH EMIs)...\n",
      "‚úÖ Data preparation complete. Final shape: (8067906, 37)\n",
      "   Unique loans: 1008504\n",
      "\n",
      "--- Step: Preprocessing and Prediction ---\n",
      "   DataFrame aligned to 30 expected features\n",
      "   Applying StandardScaler and OneHotEncoder...\n",
      "   LSTM input features: 17\n",
      "   Static dense features: 36\n",
      "   Embedding features: 2\n",
      "   Reshaping sequences for model input...\n",
      "   Reshaped data shapes:\n",
      "      LSTM: (1008504, 8, 17)\n",
      "      Static Dense: (1008504, 36)\n",
      "      Embedding: (1008504, 2)\n",
      "\n",
      "   Running model inference...\n",
      "\n",
      "‚ùå PIPELINE FAILED: in user code:\n",
      "\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 1 of layer \"model\" is incompatible with the layer: expected shape=(None, 6), found shape=(None, 36)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\101032\\AppData\\Local\\Temp\\ipykernel_22236\\1930850339.py\", line 475, in <module>\n",
      "    loan_ids, probabilities, classes, actuals = preprocess_and_predict(df_final, preprocessor, model)\n",
      "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\101032\\AppData\\Local\\Temp\\ipykernel_22236\\1930850339.py\", line 422, in preprocess_and_predict\n",
      "    y_pred_proba = model.predict(prediction_inputs, verbose=0)[:, 0]\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\101032\\AppData\\Local\\Temp\\__autograph_generated_file2d9ks3do.py\", line 18, in tf__predict_function\n",
      "    raise\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 1 of layer \"model\" is incompatible with the layer: expected shape=(None, 6), found shape=(None, 36)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================================\n",
    "# PART 1: LOS DATA LOADING AND PREPROCESSING\n",
    "# ===================================================================================\n",
    "\n",
    "def load_and_preprocess_los():\n",
    "    \"\"\"\n",
    "    Loads LOS data from PostgreSQL and applies ALL preprocessing steps \n",
    "    exactly as done in training (1_los_data_prep.ipynb).\n",
    "    \"\"\"\n",
    "    conn_string = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "    \n",
    "    try:\n",
    "        engine = sqlalchemy.create_engine(conn_string)\n",
    "        print(f\"--- Loading LOS Data from PostgreSQL Table: {LOS_TABLE_NAME} ---\")\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM \"{LOS_TABLE_NAME}\"\n",
    "        WHERE \"LOAN_DATE\" BETWEEN '{LOS_START_DATE}' AND '{LOS_END_DATE}';\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(query, con=engine)\n",
    "        print(f\"‚úÖ LOS data loaded. Shape: {df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection or query failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # === EXACT TRAINING PREPROCESSING ===\n",
    "    \n",
    "    print(\"--- Applying LOS Preprocessing (Matching Training) ---\")\n",
    "    \n",
    "    # 1. Convert PIN_CODE to string\n",
    "    if 'PIN_CODE' in df.columns:\n",
    "        df['PIN_CODE'] = df['PIN_CODE'].astype(str)\n",
    "    \n",
    "    # 2. Drop columns (matching training)\n",
    "    columns_to_drop = [\n",
    "        'CUSTOMER_ID', 'CUSTOMER_NAME', 'BRANCH_ID', 'TEMP_CUST_ID',\n",
    "        'PHONE1', 'PHONE2', 'HOUSE_NAME', 'LOCALITY', 'STREET',\n",
    "        'ALT_HOUSE_NAME', 'ALT_LOCALITY', 'ALT_STREET', 'CENTER_ID',\n",
    "        'ALT_PIN_CODE', 'MARITAL_STATUS', 'LOAN_STATUS',\n",
    "        'LOAN_STATUS_DESC', 'CLS_DT', 'CIBIL_ID', 'NPA_FLAG',\n",
    "        'NPA_FROM_DATE', 'NPA_TO_DATE', 'NPA_STATUS',\n",
    "        'OCCUPATION_ID', 'APPLICATION_ID', 'TENURE_in_months',\n",
    "        'emi_paid', 'loan_paid_percentage', 'NPA_STATUS_UPDATED',\n",
    "        'NPA_STATUS_UPDATED_1', 'NPA_STATUS_UPDATED_2', 'YEAR'\n",
    "    ]\n",
    "    df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "    \n",
    "    # 3. Drop CIBIL_SCORE and CUSTOMER_GRADING_SCORE\n",
    "    df.drop(columns=['CIBIL_SCORE', 'CUSTOMER_GRADING_SCORE'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # 4. Calculate AGE\n",
    "    df['DATE_OF_BIRTH'] = pd.to_datetime(df['DATE_OF_BIRTH'], errors='coerce')\n",
    "    df['LOAN_DATE'] = pd.to_datetime(df['LOAN_DATE'], errors='coerce')\n",
    "    df['AGE'] = (df['LOAN_DATE'] - df['DATE_OF_BIRTH']).dt.days // 365.25\n",
    "    df.drop(columns=['DATE_OF_BIRTH'], inplace=True)\n",
    "    \n",
    "    # 5. Filter CUSTOMER_FLAG (remove 'X' records)\n",
    "    df = df[df['CUSTOMER_FLAG'] != 'X'].copy()\n",
    "    \n",
    "    # 6. Drop high-cardinality columns\n",
    "    df.drop(columns=['BRANCH_NAME', 'CENTER_NAME'], errors='ignore', inplace=True)\n",
    "    \n",
    "    # 7. Impute missing values in numerical columns (USING TRAINING MEDIANS)\n",
    "    imputation_values = {\n",
    "        'TOTAL_INCOME': 20000.0,\n",
    "        'TOTAL_EXPENSE': 1760.0,\n",
    "        'AGE':  38.0\n",
    "    }\n",
    "    \n",
    "    for col, median_val in imputation_values.items():\n",
    "        if col in df.columns and df[col].isnull().sum() > 0:\n",
    "            # Use training median if available, otherwise compute from current data\n",
    "            #median_val = df[col].median() if col == 'AGE' else default_val\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "            print(f\"   Imputed {col} with value: {median_val:.2f}\")\n",
    "    \n",
    "    # 8. Impute OCCUPATION_NAME with 'UNKNOWN'\n",
    "    if 'OCCUPATION_NAME' in df.columns:\n",
    "        df['OCCUPATION_NAME'] = df['OCCUPATION_NAME'].fillna('UNKNOWN')\n",
    "    \n",
    "    # 9. Consolidate MARITAL_STATUS_NAME\n",
    "    if 'MARITAL_STATUS_NAME' in df.columns:\n",
    "        df['MARITAL_STATUS_NAME'] = df['MARITAL_STATUS_NAME'].replace(\n",
    "            ['UNMARRIED', 'SINGLE'], 'UNMARRIED/SINGLE')\n",
    "    \n",
    "    print(f\"‚úÖ LOS Preprocessing Complete. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# ===================================================================================\n",
    "# PART 2: LMS DATA LOADING AND FEATURE ENGINEERING\n",
    "# ===================================================================================\n",
    "\n",
    "def load_and_engineer_lms():\n",
    "    \"\"\"\n",
    "    Loads LMS data and applies ALL feature engineering steps \n",
    "    exactly as done in training (2_lms_data_prep.ipynb).\n",
    "    \"\"\"\n",
    "    all_lms_dfs = []\n",
    "    lms_files = sorted(glob.glob(os.path.join(NEW_LMS_FOLDER_PATH, 'test_*.xlsx')))\n",
    "    \n",
    "    if not lms_files:\n",
    "        raise FileNotFoundError(f\"No Excel files found matching 'test_*.xlsx' in {NEW_LMS_FOLDER_PATH}.\")\n",
    "    \n",
    "    print(f\"--- Loading {len(lms_files)} LMS Excel file(s) ---\")\n",
    "    for file_name in lms_files:\n",
    "        try:\n",
    "            df = pd.read_excel(file_name)\n",
    "            all_lms_dfs.append(df)\n",
    "            print(f\"   ‚úÖ Loaded: {os.path.basename(file_name)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading {os.path.basename(file_name)}: {e}\")\n",
    "    \n",
    "    df_lms = pd.concat(all_lms_dfs, ignore_index=True)\n",
    "    \n",
    "    # === EXACT TRAINING FEATURE ENGINEERING ===\n",
    "    \n",
    "    print(\"--- Applying LMS Feature Engineering ---\")\n",
    "    \n",
    "    # Core Cleaning\n",
    "    df_lms['DUE_DATE'] = pd.to_datetime(df_lms['DUE_DATE'], errors='coerce')\n",
    "    df_lms['PAID_DT'] = pd.to_datetime(df_lms['PAID_DT'], errors='coerce')\n",
    "    df_lms.loc[df_lms['STATUS'] == 1, 'PAID_DT'] = pd.NaT\n",
    "    df_lms = df_lms.sort_values(by=['LOAN_ID', 'INSTALLMENT_NO']).reset_index(drop=True)\n",
    "    \n",
    "    # Remove invalid records\n",
    "    df_lms = df_lms.loc[~((df_lms['PAID_DT'].isnull()) & (df_lms['STATUS'] != 1))].reset_index(drop=True)\n",
    "    \n",
    "    # Core Features\n",
    "    df_lms['DAYS_LATE'] = (df_lms['PAID_DT'] - df_lms['DUE_DATE']).dt.days\n",
    "    df_lms['DAYS_BETWEEN_DUE_DATES'] = df_lms.groupby('LOAN_ID')['DUE_DATE'].diff().dt.days\n",
    "    \n",
    "    # Repayment Schedule Category\n",
    "    df_lms['REPAYMENT_SCHEDULE_CAT'] = np.select(\n",
    "        [\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'].isnull(),\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'].isin([28, 29, 30, 31]),\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'] == 7,\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'] == 14,\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'] == 56\n",
    "        ],\n",
    "        ['Initial', 'Monthly', 'Weekly', 'Bi-Weekly', 'Bi-Monthly'],\n",
    "        default='Other'\n",
    "    )\n",
    "    \n",
    "    # Loan Schedule Type\n",
    "    has_weekly = df_lms.groupby('LOAN_ID')['REPAYMENT_SCHEDULE_CAT'].transform(\n",
    "        lambda x: 'Weekly' in x.unique())\n",
    "    has_monthly = df_lms.groupby('LOAN_ID')['REPAYMENT_SCHEDULE_CAT'].transform(\n",
    "        lambda x: 'Monthly' in x.unique())\n",
    "    mode_schedule = df_lms.groupby('LOAN_ID')['REPAYMENT_SCHEDULE_CAT'].transform(\n",
    "        lambda x: x.mode()[0] if not x.mode().empty else 'Initial')\n",
    "    df_lms['LOAN_SCHEDULE_TYPE'] = np.where((has_weekly) & (has_monthly), 'Hybrid', mode_schedule)\n",
    "    \n",
    "    # Payment Features\n",
    "    df_lms['IS_UNPAID'] = np.where(df_lms['STATUS'] == 1, 1, 0)\n",
    "    df_lms['IS_DAYS_LATE_MISSING'] = df_lms['DAYS_LATE'].isna().astype(int)\n",
    "    df_lms['DAYS_LATE'] = df_lms['DAYS_LATE'].fillna(0)\n",
    "    df_lms['PAID_RATIO'] = (df_lms['PAID_AMOUNT'] / df_lms['INSTALLMENT_AMOUNT']).clip(upper=1.0)\n",
    "    df_lms['DELTA_DAYS_LATE'] = df_lms.groupby('LOAN_ID')['DAYS_LATE'].diff().fillna(0)\n",
    "    \n",
    "    # CRITICAL: Zero out DELTA_DAYS_LATE for unpaid EMIs (matching training)\n",
    "    df_lms.loc[df_lms['IS_UNPAID'] == 1, 'DELTA_DAYS_LATE'] = 0\n",
    "    \n",
    "    # Current EMI Behavior Label\n",
    "    conditions_behavior = [\n",
    "        (df_lms['STATUS'] == 1) |\n",
    "        ((df_lms['STATUS'] == 0) & (df_lms['DAYS_LATE'] > GRACE_PERIOD_DAYS)) |\n",
    "        ((df_lms['STATUS'] == 2) & (\n",
    "            (df_lms['DAYS_LATE'] > GRACE_PERIOD_DAYS) |\n",
    "            (df_lms['PAID_AMOUNT'] / df_lms['INSTALLMENT_AMOUNT'] < PAID_PERCENTAGE_THRESHOLD)\n",
    "        )),\n",
    "        (df_lms['STATUS'] == 0) & (df_lms['DAYS_LATE'] <= GRACE_PERIOD_DAYS),\n",
    "        (df_lms['STATUS'] == 2) & (df_lms['DAYS_LATE'] <= GRACE_PERIOD_DAYS) &\n",
    "        (df_lms['PAID_AMOUNT'] / df_lms['INSTALLMENT_AMOUNT'] >= PAID_PERCENTAGE_THRESHOLD)\n",
    "    ]\n",
    "    choices_behavior = [1, 0, 0]\n",
    "    df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] = np.select(\n",
    "        conditions_behavior, choices_behavior, default=-1).astype(np.int8)\n",
    "    \n",
    "    # Composite Risk and Payment Score\n",
    "    df_lms['REMAINING_EMI_RATIO'] = 1 - df_lms[\"PAID_RATIO\"]\n",
    "    df_lms['COMPOSITE_RISK'] = df_lms['DAYS_LATE'] + (df_lms['REMAINING_EMI_RATIO'] * 10)\n",
    "    \n",
    "    conditions_score = [\n",
    "        df_lms['IS_UNPAID'] == 1,\n",
    "        (df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] == 1) & (df_lms['IS_UNPAID'] == 0),\n",
    "        (df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] == 0) & (df_lms['DAYS_LATE'] > 0) &\n",
    "        (df_lms['DAYS_LATE'] <= GRACE_PERIOD_DAYS) & (df_lms['IS_UNPAID'] == 0),\n",
    "        (df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] == 0) & (df_lms['DAYS_LATE'] <= 0) &\n",
    "        (df_lms['IS_UNPAID'] == 0)\n",
    "    ]\n",
    "    \n",
    "    choices_score = [\n",
    "        -100,\n",
    "        np.maximum(0.0, np.minimum(0.30, 0.30 - (df_lms['COMPOSITE_RISK'] * 0.03))),\n",
    "        1.0 / (1 + df_lms['COMPOSITE_RISK']),\n",
    "        1.5 + (np.abs(df_lms['DAYS_LATE']) / 10)\n",
    "    ]\n",
    "    df_lms['PAYMENT_SCORE'] = np.select(conditions_score, choices_score, default=0).astype(np.float32)\n",
    "    \n",
    "    rank_choices = [4, 3, 2, 1]\n",
    "    df_lms['PAYMENT_SCORE_RANK'] = np.select(conditions_score, rank_choices).astype(np.int8)\n",
    "    \n",
    "    # === VALIDATION MODE: Extract actual targets before trimming ===\n",
    "    actual_targets = df_lms.groupby('LOAN_ID').tail(TARGET_BACKTEST_EMI).copy()\n",
    "    actual_targets = actual_targets[['LOAN_ID', 'CURRENT_EMI_BEHAVIOR_LABEL']].rename(\n",
    "        columns={'CURRENT_EMI_BEHAVIOR_LABEL': 'ACTUAL_NEXT_EMI_ISSUE'}\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ LMS Feature Engineering Complete. Shape: {df_lms.shape}\")\n",
    "    return df_lms, actual_targets\n",
    "\n",
    "# ===================================================================================\n",
    "# PART 3: DATA MERGING AND FINAL PREPARATION\n",
    "# ===================================================================================\n",
    "\n",
    "def merge_and_finalize_data(df_los, df_lms_full, df_actual_targets, category_mappings):\n",
    "    \"\"\"\n",
    "    Merges LOS and LMS data, applies final feature engineering,\n",
    "    and performs back-test trimming exactly as in training (3_hybrid_data_prep_optimized.ipynb).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. VALIDATION MODE: Trim the last EMI (EMI N) from sequence\n",
    "    print(f\"\\n--- Back-testing Mode: Trimming last {TARGET_BACKTEST_EMI} EMI(s) ---\")\n",
    "    indices_to_drop = df_lms_full.groupby('LOAN_ID').tail(TARGET_BACKTEST_EMI).index\n",
    "    df_lms = df_lms_full.drop(indices_to_drop).reset_index(drop=True)\n",
    "    print(f\"   LMS records after trimming: {df_lms.shape[0]}\")\n",
    "    \n",
    "    # 2. Rename LMS columns to avoid conflicts\n",
    "    RENAME_MAP_LMS = {\n",
    "        'LOAN_AMOUNT': 'LOAN_AMOUNT_LMS',\n",
    "        'TENURE': 'TENURE_LMS',\n",
    "        'INTEREST_RATE': 'INTEREST_RATE_LMS'\n",
    "    }\n",
    "    df_lms.rename(columns=RENAME_MAP_LMS, inplace=True)\n",
    "    \n",
    "    # 3. Rename LOS columns\n",
    "    RENAME_MAP_LOS = {\n",
    "        'LOAN_AMOUNT': 'LOAN_AMOUNT_STATIC',\n",
    "        'TENURE': 'TENURE_STATIC',\n",
    "        'INTEREST_RATE': 'INTEREST_RATE_STATIC'\n",
    "    }\n",
    "    df_los.rename(columns=RENAME_MAP_LOS, inplace=True)\n",
    "    \n",
    "    # 4. Merge\n",
    "    df_combined = pd.merge(df_los, df_lms, on='LOAN_ID', how='inner', suffixes=('_static', '_lms'))\n",
    "    print(f\"   Combined shape after merge: {df_combined.shape}\")\n",
    "    \n",
    "    # 5. Clean up column names\n",
    "    df_combined.rename(columns={\n",
    "        \"LOAN_DATE_static\": \"LOAN_DATE\",\n",
    "        \"LOAN_AMOUNT_STATIC\": \"LOAN_AMOUNT\",\n",
    "        \"TENURE_STATIC\": \"TENURE\",\n",
    "        \"INTEREST_RATE_STATIC\": \"INTEREST_RATE\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # 6. Drop unnecessary columns\n",
    "    cols_to_drop = [\n",
    "        'LOAN_DATE_lms', 'LOAN_AMOUNT_lms', 'TENURE_lms', 'INTEREST_RATE_lms',\n",
    "        'DISBURSED_AMOUNT', 'PIN_CODE', 'IS_DAYS_LATE_MISSING', 'STATUS',\n",
    "        'PAID_DT', 'PAID_AMOUNT', 'REMAINING_EMI_RATIO', 'TENURE', 'INTEREST_RATE'\n",
    "    ]\n",
    "    df_combined.drop(columns=[c for c in cols_to_drop if c in df_combined.columns],\n",
    "                     errors='ignore', inplace=True)\n",
    "    \n",
    "    # 7. Impute DAYS_BETWEEN_DUE_DATES\n",
    "    df_combined[\"DAYS_BETWEEN_DUE_DATES\"].fillna(0, inplace=True)\n",
    "    \n",
    "    # 8. Rolling Feature (RECENT_PAYMENT_SCORE) - WITH PROPER SHIFTING\n",
    "    print(\"   Creating RECENT_PAYMENT_SCORE (rolling window feature)...\")\n",
    "    df_combined['RECENT_PAYMENT_SCORE'] = df_combined.groupby('LOAN_ID')['PAYMENT_SCORE'].rolling(\n",
    "        window=ROLLING_WINDOW_SIZE, min_periods=1\n",
    "    ).mean().reset_index(level=0, drop=True).astype(np.float32)\n",
    "    \n",
    "    # CRITICAL: Shift to prevent leakage\n",
    "    df_combined['RECENT_PAYMENT_SCORE'] = df_combined.groupby('LOAN_ID')['RECENT_PAYMENT_SCORE'].shift(1)\n",
    "    \n",
    "    # Use TRAINING mean for imputation\n",
    "    overall_mean_score = -39.098625  # From training\n",
    "    df_combined['RECENT_PAYMENT_SCORE'].fillna(overall_mean_score, inplace=True)\n",
    "    \n",
    "    # 9. Custom Encoding\n",
    "    print(\"   Applying custom encoding for CUSTOMER_FLAG, OCCUPATION_NAME, LOAN_PURPOSE...\")\n",
    "    df_combined['CUSTOMER_FLAG_ENCODED'] = df_combined['CUSTOMER_FLAG'].map(FLAG_ORDER).fillna(0).astype(np.int8)\n",
    "    \n",
    "    # Use training category mappings for consistent encoding\n",
    "    if category_mappings['occupation'] and category_mappings['purpose']:\n",
    "        occupation_to_code = {v: k+1 for k, v in category_mappings['occupation'].items()}\n",
    "        purpose_to_code = {v: k+1 for k, v in category_mappings['purpose'].items()}\n",
    "        \n",
    "        df_combined['OCCUPATION_NAME_ENCODED'] = df_combined['OCCUPATION_NAME'].astype(str).map(\n",
    "            occupation_to_code).fillna(0).astype(np.int16)\n",
    "        df_combined['LOAN_PURPOSE_ENCODED'] = df_combined['LOAN_PURPOSE'].astype(str).map(\n",
    "            purpose_to_code).fillna(0).astype(np.int16)\n",
    "        \n",
    "        print(f\"      OCCUPATION_NAME: {df_combined['OCCUPATION_NAME_ENCODED'].nunique()} unique codes\")\n",
    "        print(f\"      LOAN_PURPOSE: {df_combined['LOAN_PURPOSE_ENCODED'].nunique()} unique codes\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è WARNING: Using fallback encoding (may be inconsistent)\")\n",
    "    \n",
    "    df_combined.drop(columns=['CUSTOMER_FLAG', 'OCCUPATION_NAME', 'LOAN_PURPOSE', 'LOAN_DATE'],\n",
    "                     errors='ignore', inplace=True)\n",
    "    \n",
    "    # 10. OHE Sequential Categorical Feature\n",
    "    print(\"   One-hot encoding REPAYMENT_SCHEDULE_CAT...\")\n",
    "    df_combined = pd.get_dummies(df_combined, columns=['REPAYMENT_SCHEDULE_CAT'], prefix='REPAYMENT_CAT')\n",
    "    \n",
    "    # 11. Filter for the latest sequence for prediction\n",
    "    print(\"   Filtering to keep only necessary sequence (last MAX_SEQUENCE_LENGTH EMIs)...\")\n",
    "    df_combined['SEQUENCE_COUNT'] = df_combined.groupby('LOAN_ID').cumcount() + 1\n",
    "    df_combined['REVERSE_SEQUENCE_COUNT'] = df_combined.groupby('LOAN_ID')['SEQUENCE_COUNT'].transform(\n",
    "        'max') - df_combined['SEQUENCE_COUNT']\n",
    "    df_combined_filtered = df_combined[df_combined['REVERSE_SEQUENCE_COUNT'] < MAX_SEQUENCE_LENGTH].copy()\n",
    "    df_combined_filtered.drop(columns=['SEQUENCE_COUNT', 'REVERSE_SEQUENCE_COUNT'], inplace=True)\n",
    "    \n",
    "    # 12. Merge Actual Target for Validation\n",
    "    df_combined_filtered = pd.merge(df_combined_filtered, df_actual_targets, on='LOAN_ID', how='left')\n",
    "    df_combined_filtered['ACTUAL_NEXT_EMI_ISSUE'] = df_combined_filtered.groupby(\n",
    "        'LOAN_ID')['ACTUAL_NEXT_EMI_ISSUE'].transform('max')\n",
    "    df_combined_filtered.dropna(subset=['ACTUAL_NEXT_EMI_ISSUE'], inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ Data preparation complete. Final shape: {df_combined_filtered.shape}\")\n",
    "    print(f\"   Unique loans: {df_combined_filtered['LOAN_ID'].nunique()}\")\n",
    "    \n",
    "    return df_combined_filtered\n",
    "\n",
    "# ===================================================================================\n",
    "# PART 4: PREPROCESSING AND PREDICTION\n",
    "# ===================================================================================\n",
    "\n",
    "def preprocess_and_predict(df_full, preprocessor, model):\n",
    "    \"\"\"\n",
    "    Applies preprocessor, reshapes sequences, and runs inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n--- Step: Preprocessing and Prediction ---\")\n",
    "    \n",
    "    # Get expected features from preprocessor\n",
    "    expected_features = list(preprocessor.feature_names_in_)\n",
    "    \n",
    "    # Align DataFrame columns with expected features (safe approach)\n",
    "    df_aligned = df_full.reindex(columns=expected_features, fill_value=0)\n",
    "    print(f\"   DataFrame aligned to {len(expected_features)} expected features\")\n",
    "    \n",
    "    # Apply ColumnTransformer\n",
    "    print(\"   Applying StandardScaler and OneHotEncoder...\")\n",
    "    X_scaled_ohe = preprocessor.transform(df_aligned)\n",
    "    ALL_FINAL_COLS = list(preprocessor.get_feature_names_out())\n",
    "    \n",
    "    # Reconstruct DataFrame\n",
    "    X_predict_df = pd.DataFrame(X_scaled_ohe, columns=ALL_FINAL_COLS, index=df_full.index)\n",
    "    \n",
    "    # Add back non-transformed columns\n",
    "    X_predict_df['LOAN_ID'] = df_full['LOAN_ID'].values\n",
    "    X_predict_df['ACTUAL_NEXT_EMI_ISSUE'] = df_full['ACTUAL_NEXT_EMI_ISSUE'].values\n",
    "    for col in STATIC_EMBEDDING_COLS:\n",
    "        X_predict_df[col] = df_full[col].values\n",
    "    \n",
    "    # Define input feature sets with correct prefixes\n",
    "    training_ohe_cols = [c for c in expected_features if c.startswith('REPAYMENT_CAT_')]\n",
    "    STATIC_COLS_NUMERICAL_FINAL = STATIC_COLS_NUMERICAL + ['CUSTOMER_FLAG_ENCODED']\n",
    "    \n",
    "    LSTM_INPUT_COLS_FINAL = [f\"num__{c}\" for c in SEQUENTIAL_COLS_NUMERICAL] + \\\n",
    "                            [f\"num__{c}\" for c in training_ohe_cols]\n",
    "    \n",
    "    STATIC_DENSE_COLS_FINAL = [f\"num__{c}\" for c in STATIC_COLS_NUMERICAL_FINAL] + \\\n",
    "                              [c for c in ALL_FINAL_COLS if c.startswith('cat__')]\n",
    "    \n",
    "    print(f\"   LSTM input features: {len(LSTM_INPUT_COLS_FINAL)}\")\n",
    "    print(f\"   Static dense features: {len(STATIC_DENSE_COLS_FINAL)}\")\n",
    "    print(f\"   Embedding features: {len(STATIC_EMBEDDING_COLS)}\")\n",
    "    \n",
    "    # Sequence Reshaping\n",
    "    def reshape_for_prediction(X_df, lstm_cols, static_dense_cols, embedding_cols, max_len):\n",
    "        grouped = X_df.groupby('LOAN_ID')\n",
    "        loan_ids = list(grouped.groups.keys())\n",
    "        \n",
    "        X_lstm = np.zeros((len(loan_ids), max_len, len(lstm_cols)), dtype=np.float32)\n",
    "        X_static_dense = np.zeros((len(loan_ids), len(static_dense_cols)), dtype=np.float32)\n",
    "        X_static_embed = np.zeros((len(loan_ids), len(embedding_cols)), dtype=np.int16)\n",
    "        y_actual = np.zeros((len(loan_ids),), dtype=np.int8)\n",
    "        \n",
    "        for i, loan_id in enumerate(loan_ids):\n",
    "            loan_data = grouped.get_group(loan_id)\n",
    "            sequence = loan_data[lstm_cols].values\n",
    "            \n",
    "            # Pad or truncate sequence\n",
    "            if len(sequence) >= max_len:\n",
    "                X_lstm[i, :, :] = sequence[-max_len:]\n",
    "            else:\n",
    "                X_lstm[i, -len(sequence):, :] = sequence\n",
    "            \n",
    "            # Get static features from last record\n",
    "            last_record = loan_data.iloc[-1]\n",
    "            X_static_dense[i, :] = last_record[static_dense_cols].values\n",
    "            X_static_embed[i, :] = last_record[embedding_cols].values.astype(np.int16)\n",
    "            y_actual[i] = last_record['ACTUAL_NEXT_EMI_ISSUE'].astype(np.int8)\n",
    "        \n",
    "        return X_lstm, X_static_dense, X_static_embed, loan_ids, y_actual\n",
    "    \n",
    "    # Execute reshaping\n",
    "    print(\"   Reshaping sequences for model input...\")\n",
    "    X_predict_lstm, X_predict_static_dense, X_predict_static_embed, prediction_loan_ids, y_actual = \\\n",
    "        reshape_for_prediction(X_predict_df, LSTM_INPUT_COLS_FINAL, STATIC_DENSE_COLS_FINAL,\n",
    "                              STATIC_EMBEDDING_COLS, MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    print(f\"   Reshaped data shapes:\")\n",
    "    print(f\"      LSTM: {X_predict_lstm.shape}\")\n",
    "    print(f\"      Static Dense: {X_predict_static_dense.shape}\")\n",
    "    print(f\"      Embedding: {X_predict_static_embed.shape}\")\n",
    "    \n",
    "    # Model Inference\n",
    "    print(\"\\n   Running model inference...\")\n",
    "    prediction_inputs = {\n",
    "        'lstm_input': X_predict_lstm,\n",
    "        'static_dense_input': X_predict_static_dense,\n",
    "        'embedding_input': X_predict_static_embed\n",
    "    }\n",
    "    \n",
    "    y_pred_proba = model.predict(prediction_inputs, verbose=0)[:, 0]\n",
    "    y_pred_class = (y_pred_proba > 0.5).astype(np.int8)\n",
    "    print(f\"‚úÖ Prediction complete for {len(prediction_loan_ids)} unique loans.\")\n",
    "    return prediction_loan_ids, y_pred_proba, y_pred_class, y_actual\n",
    "\n",
    "# ===================================================================================\n",
    "# PART 5: EVALUATION AND REPORTING\n",
    "# ===================================================================================\n",
    "\n",
    "def evaluate_performance(y_true, y_pred_class, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Generates a professional performance report for the back-test.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìà BACK-TEST PERFORMANCE REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred_class)\n",
    "    cm = confusion_matrix(y_true, y_pred_class)\n",
    "    \n",
    "    print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(pd.DataFrame(cm, \n",
    "                       index=['Actual Paid', 'Actual Default'], \n",
    "                       columns=['Pred Paid', 'Pred Default']))\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred_class, target_names=['Paid (0)', 'Not Paid (1)']))\n",
    "    \n",
    "    # Senior insight: Financial models care about the \"Cost of False Negatives\"\n",
    "    fn = cm[1, 0]\n",
    "    print(f\"‚ö†Ô∏è  Missed Defaults (False Negatives): {fn}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ===================================================================================\n",
    "# MAIN EXECUTION PIPELINE\n",
    "# ===================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 1. Load Model and Artifacts\n",
    "        preprocessor, model, category_mappings = load_artifacts()\n",
    "\n",
    "        # 2. Extract Static Data (LOS)\n",
    "        df_los = load_and_preprocess_los()\n",
    "\n",
    "        # 3. Extract Sequential Data (LMS)\n",
    "        df_lms_full, df_actual_targets = load_and_engineer_lms()\n",
    "\n",
    "        # 4. Merge and Align for Production\n",
    "        df_final = merge_and_finalize_data(df_los, df_lms_full, df_actual_targets, category_mappings)\n",
    "\n",
    "        # 5. Inference\n",
    "        loan_ids, probabilities, classes, actuals = preprocess_and_predict(df_final, preprocessor, model)\n",
    "\n",
    "        # 6. Performance Evaluation (Back-testing)\n",
    "        evaluate_performance(actuals, classes, probabilities)\n",
    "\n",
    "        # 7. Consolidated Results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'LOAN_ID': loan_ids,\n",
    "            'ACTUAL_VALUE': actuals,\n",
    "            'PREDICTED_CLASS': classes,\n",
    "            'DEFAULT_PROBABILITY': probabilities\n",
    "        })\n",
    "        \n",
    "        # Mapping for readability\n",
    "        results_df['PREDICTED_STATUS'] = results_df['PREDICTED_CLASS'].map(TWO_CLASS_STATUS_MAP)\n",
    "        results_df['ACTUAL_STATUS'] = results_df['ACTUAL_VALUE'].map(TWO_CLASS_STATUS_MAP)\n",
    "\n",
    "        # 8. Save results\n",
    "        output_file = \"oct_2025_backtest_results.csv\"\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"‚úÖ Results successfully exported to: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå PIPELINE FAILED: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4551bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_local_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
