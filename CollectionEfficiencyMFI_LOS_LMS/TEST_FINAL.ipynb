{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e226f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HYBRID LSTM MODEL - PRODUCTION PREDICTION PIPELINE\n",
    "# Purpose: Back-testing and Real-time Prediction for EMI Payment Default Risk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import glob\n",
    "import os\n",
    "import sqlalchemy\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5218612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ===================================================================================\n",
    "\n",
    "# Database Configuration\n",
    "DB_CONFIG = {\n",
    "    'host': '10.192.5.43',\n",
    "    'port': 5432,\n",
    "    'database': 'postgres',\n",
    "    'user': 'ml_db',\n",
    "    'password': 'pass%401234'\n",
    "}\n",
    "\n",
    "# Data Configuration\n",
    "LOS_TABLE_NAME = \"Ashirvad\"\n",
    "LOS_START_DATE = '2024-01-01'\n",
    "LOS_END_DATE = '2024-12-31'\n",
    "NEW_LMS_FOLDER_PATH = \"TestData_Oct2025prediction\"\n",
    "\n",
    "# Model Configuration\n",
    "MAX_SEQUENCE_LENGTH = 8\n",
    "ROLLING_WINDOW_SIZE = 3\n",
    "GRACE_PERIOD_DAYS = 2\n",
    "PAID_PERCENTAGE_THRESHOLD = 0.90\n",
    "\n",
    "# Validation Configuration\n",
    "TARGET_BACKTEST_EMI = 1  # Predict EMI N using data up to EMI N-1\n",
    "\n",
    "# Status Mapping\n",
    "TWO_CLASS_STATUS_MAP = {0: 'Paid', 1: 'Not Paid'}\n",
    "FLAG_ORDER = {'A': 4, 'B': 3, 'C': 2, 'D': 1}\n",
    "\n",
    "# Feature Definitions (MUST MATCH TRAINING EXACTLY)\n",
    "SEQUENTIAL_COLS_NUMERICAL = [\n",
    "    'INSTALLMENT_NO', 'INSTALLMENT_AMOUNT', 'DAYS_LATE', 'DAYS_BETWEEN_DUE_DATES',\n",
    "    'PAID_RATIO', 'DELTA_DAYS_LATE', 'PAYMENT_SCORE', 'COMPOSITE_RISK',\n",
    "    'RECENT_PAYMENT_SCORE', 'PAYMENT_SCORE_RANK', 'IS_UNPAID', 'CURRENT_EMI_BEHAVIOR_LABEL'\n",
    "]\n",
    "\n",
    "STATIC_COLS_NUMERICAL = ['TOTAL_INCOME', 'TOTAL_EXPENSE', 'LOAN_AMOUNT', 'AGE', 'CYCLE']\n",
    "STATIC_COLS_OHE = ['MARITAL_STATUS_NAME', 'STATE_NAME', 'LOAN_SCHEDULE_TYPE']\n",
    "STATIC_EMBEDDING_COLS = ['OCCUPATION_NAME_ENCODED', 'LOAN_PURPOSE_ENCODED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498c97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artifacts():\n",
    "    \"\"\"\n",
    "    Load all required model artifacts. \n",
    "    Strict Version: Will raise a FileNotFoundError and stop execution if ANY file is missing.\n",
    "    \"\"\"\n",
    "    # 1. Define required files\n",
    "    artifact_paths = {\n",
    "        'preprocessor': 'preprocessor.pkl',\n",
    "        'model': 'hybrid_lstm_model.h5',\n",
    "        'mappings': 'embedding_category_mappings.pkl'\n",
    "    }\n",
    "\n",
    "    # 2. Pre-check existence for clear reporting\n",
    "    missing_files = [path for path in artifact_paths.values() if not os.path.exists(path)]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(\"\\n\" + \"!\" * 60)\n",
    "        print(\"‚ùå CRITICAL ERROR: MISSION-CRITICAL ARTIFACTS MISSING\")\n",
    "        print(f\"The following required files were not found: {missing_files}\")\n",
    "        print(\"Pipeline execution halted to prevent incorrect predictions.\")\n",
    "        print(\"!\" * 60 + \"\\n\")\n",
    "        # Explicitly raise error to stop the script\n",
    "        raise FileNotFoundError(f\"Missing artifacts: {missing_files}\")\n",
    "\n",
    "    # 3. Load artifacts strictly\n",
    "    try:\n",
    "        preprocessor = joblib.load(artifact_paths['preprocessor'])\n",
    "        model = load_model(artifact_paths['model'], compile=False)\n",
    "        category_mappings = joblib.load(artifact_paths['mappings'])\n",
    "        \n",
    "        print(\"‚úÖ Success: Preprocessor, Model, and Mappings loaded.\")\n",
    "        return preprocessor, model, category_mappings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: Artifacts present but failed to load. Check for corruption or version mismatch: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ca317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "‚úÖ Success: Preprocessor, Model, and Mappings loaded.\n",
      "--- Loading LOS Data from PostgreSQL Table: Ashirvad ---\n",
      "‚úÖ LOS data loaded. Shape: (1111834, 52)\n",
      "--- Applying LOS Preprocessing (Matching Training) ---\n",
      "   Imputed TOTAL_INCOME with value: 20000.00\n",
      "   Imputed TOTAL_EXPENSE with value: 1760.00\n",
      "‚úÖ LOS Preprocessing Complete. Shape: (1095563, 16)\n",
      "--- Loading 24 LMS Excel file(s) ---\n",
      "   ‚úÖ Loaded: test_001.xlsx\n",
      "   ‚úÖ Loaded: test_002.xlsx\n",
      "   ‚úÖ Loaded: test_003.xlsx\n",
      "   ‚úÖ Loaded: test_004.xlsx\n",
      "   ‚úÖ Loaded: test_005.xlsx\n",
      "   ‚úÖ Loaded: test_006.xlsx\n",
      "   ‚úÖ Loaded: test_007.xlsx\n",
      "   ‚úÖ Loaded: test_008.xlsx\n",
      "   ‚úÖ Loaded: test_009.xlsx\n",
      "   ‚úÖ Loaded: test_010.xlsx\n",
      "   ‚úÖ Loaded: test_011.xlsx\n",
      "   ‚úÖ Loaded: test_012.xlsx\n",
      "   ‚úÖ Loaded: test_013.xlsx\n",
      "   ‚úÖ Loaded: test_014.xlsx\n",
      "   ‚úÖ Loaded: test_015.xlsx\n",
      "   ‚úÖ Loaded: test_016.xlsx\n",
      "   ‚úÖ Loaded: test_017.xlsx\n",
      "   ‚úÖ Loaded: test_018.xlsx\n",
      "   ‚úÖ Loaded: test_019.xlsx\n",
      "   ‚úÖ Loaded: test_020.xlsx\n",
      "   ‚úÖ Loaded: test_021.xlsx\n",
      "   ‚úÖ Loaded: test_022.xlsx\n",
      "   ‚úÖ Loaded: test_023.xlsx\n",
      "   ‚úÖ Loaded: test_024.xlsx\n",
      "--- Applying LMS Feature Engineering ---\n",
      "‚úÖ LMS Feature Engineering Complete. Shape: (18855547, 26)\n",
      "\n",
      "--- Back-testing Mode: Trimming last 1 EMI(s) ---\n",
      "   LMS records after trimming: 17831866\n",
      "\n",
      "‚ùå PIPELINE FAILED: Unable to allocate 804. MiB for an array with shape (6, 17557669) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\101032\\AppData\\Local\\Temp\\ipykernel_12472\\719006584.py\", line 648, in <module>\n",
      "    df_final = merge_and_finalize_data(df_los, df_lms_full, df_actual_targets, category_mappings)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\101032\\AppData\\Local\\Temp\\ipykernel_12472\\719006584.py\", line 248, in merge_and_finalize_data\n",
      "    df_combined = pd.merge(df_los, df_lms, on='LOAN_ID', how='inner', suffixes=('_static', '_lms'))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 183, in merge\n",
      "    return op.get_result(copy=copy)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 885, in get_result\n",
      "    result = self._reindex_and_concat(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 860, in _reindex_and_concat\n",
      "    rmgr = right._mgr.reindex_indexer(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 670, in reindex_indexer\n",
      "    new_blocks = [\n",
      "                 ^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 671, in <listcomp>\n",
      "    blk.take_nd(\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1061, in take_nd\n",
      "    new_values = algos.take_nd(\n",
      "                 ^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py\", line 118, in take_nd\n",
      "    return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\VS_CODE_PROJECTS\\CollectionEfficiencyMFI_LOS_LMS\\env_local_311\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py\", line 158, in _take_nd_ndarray\n",
      "    out = np.empty(out_shape, dtype=dtype)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 804. MiB for an array with shape (6, 17557669) and data type float64\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================================\n",
    "# PART 1: LOS DATA LOADING AND PREPROCESSING\n",
    "# ===================================================================================\n",
    "\n",
    "def load_and_preprocess_los():\n",
    "    \"\"\"\n",
    "    Loads LOS data from PostgreSQL and applies ALL preprocessing steps \n",
    "    exactly as done in training (1_los_data_prep.ipynb).\n",
    "    \"\"\"\n",
    "    conn_string = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "    \n",
    "    try:\n",
    "        engine = sqlalchemy.create_engine(conn_string)\n",
    "        print(f\"--- Loading LOS Data from PostgreSQL Table: {LOS_TABLE_NAME} ---\")\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM \"{LOS_TABLE_NAME}\"\n",
    "        WHERE \"LOAN_DATE\" BETWEEN '{LOS_START_DATE}' AND '{LOS_END_DATE}';\n",
    "        \"\"\"\n",
    "        df = pd.read_sql_query(query, con=engine)\n",
    "        print(f\"‚úÖ LOS data loaded. Shape: {df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection or query failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # === EXACT TRAINING PREPROCESSING ===\n",
    "    \n",
    "    print(\"--- Applying LOS Preprocessing (Matching Training) ---\")\n",
    "    \n",
    "    # 1. Convert PIN_CODE to string\n",
    "    if 'PIN_CODE' in df.columns:\n",
    "        df['PIN_CODE'] = df['PIN_CODE'].astype(str)\n",
    "    \n",
    "    # 2. Drop columns (matching training)\n",
    "    columns_to_drop = [\n",
    "        'CUSTOMER_ID', 'CUSTOMER_NAME', 'BRANCH_ID', 'TEMP_CUST_ID',\n",
    "        'PHONE1', 'PHONE2', 'HOUSE_NAME', 'LOCALITY', 'STREET',\n",
    "        'ALT_HOUSE_NAME', 'ALT_LOCALITY', 'ALT_STREET', 'CENTER_ID',\n",
    "        'ALT_PIN_CODE', 'MARITAL_STATUS', 'LOAN_STATUS',\n",
    "        'LOAN_STATUS_DESC', 'CLS_DT', 'CIBIL_ID', 'NPA_FLAG',\n",
    "        'NPA_FROM_DATE', 'NPA_TO_DATE', 'NPA_STATUS',\n",
    "        'OCCUPATION_ID', 'APPLICATION_ID', 'TENURE_in_months',\n",
    "        'emi_paid', 'loan_paid_percentage', 'NPA_STATUS_UPDATED',\n",
    "        'NPA_STATUS_UPDATED_1', 'NPA_STATUS_UPDATED_2', 'YEAR'\n",
    "    ]\n",
    "    df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "    \n",
    "    # 3. Drop CIBIL_SCORE and CUSTOMER_GRADING_SCORE\n",
    "    df.drop(columns=['CIBIL_SCORE', 'CUSTOMER_GRADING_SCORE'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # 4. Calculate AGE\n",
    "    df['DATE_OF_BIRTH'] = pd.to_datetime(df['DATE_OF_BIRTH'], errors='coerce')\n",
    "    df['LOAN_DATE'] = pd.to_datetime(df['LOAN_DATE'], errors='coerce')\n",
    "    df['AGE'] = (df['LOAN_DATE'] - df['DATE_OF_BIRTH']).dt.days // 365.25\n",
    "    df.drop(columns=['DATE_OF_BIRTH'], inplace=True)\n",
    "    \n",
    "    # 5. Filter CUSTOMER_FLAG (remove 'X' records)\n",
    "    df = df[df['CUSTOMER_FLAG'] != 'X'].copy()\n",
    "    \n",
    "    # 6. Drop high-cardinality columns\n",
    "    df.drop(columns=['BRANCH_NAME', 'CENTER_NAME'], errors='ignore', inplace=True)\n",
    "    \n",
    "    # 7. Impute missing values in numerical columns (USING TRAINING MEDIANS)\n",
    "    imputation_values = {\n",
    "        'TOTAL_INCOME': 20000.0,\n",
    "        'TOTAL_EXPENSE': 1760.0,\n",
    "        'AGE':  38.0\n",
    "    }\n",
    "    \n",
    "    for col, median_val in imputation_values.items():\n",
    "        if col in df.columns and df[col].isnull().sum() > 0:\n",
    "            # Use training median if available, otherwise compute from current data\n",
    "            #median_val = df[col].median() if col == 'AGE' else default_val\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "            print(f\"   Imputed {col} with value: {median_val:.2f}\")\n",
    "    \n",
    "    # 8. Impute OCCUPATION_NAME with 'UNKNOWN'\n",
    "    if 'OCCUPATION_NAME' in df.columns:\n",
    "        df['OCCUPATION_NAME'] = df['OCCUPATION_NAME'].fillna('UNKNOWN')\n",
    "    \n",
    "    # 9. Consolidate MARITAL_STATUS_NAME\n",
    "    if 'MARITAL_STATUS_NAME' in df.columns:\n",
    "        df['MARITAL_STATUS_NAME'] = df['MARITAL_STATUS_NAME'].replace(\n",
    "            ['UNMARRIED', 'SINGLE'], 'UNMARRIED/SINGLE')\n",
    "    \n",
    "    print(f\"‚úÖ LOS Preprocessing Complete. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# ===================================================================================\n",
    "# PART 2: LMS DATA LOADING AND FEATURE ENGINEERING\n",
    "# ===================================================================================\n",
    "\n",
    "def load_and_engineer_lms():\n",
    "    \"\"\"\n",
    "    Loads LMS data and applies ALL feature engineering steps \n",
    "    exactly as done in training (2_lms_data_prep.ipynb).\n",
    "    \"\"\"\n",
    "    all_lms_dfs = []\n",
    "    lms_files = sorted(glob.glob(os.path.join(NEW_LMS_FOLDER_PATH, 'test_*.xlsx')))\n",
    "    \n",
    "    if not lms_files:\n",
    "        raise FileNotFoundError(f\"No Excel files found matching 'test_*.xlsx' in {NEW_LMS_FOLDER_PATH}.\")\n",
    "    \n",
    "    print(f\"--- Loading {len(lms_files)} LMS Excel file(s) ---\")\n",
    "    for file_name in lms_files:\n",
    "        try:\n",
    "            df = pd.read_excel(file_name)\n",
    "            all_lms_dfs.append(df)\n",
    "            print(f\"   ‚úÖ Loaded: {os.path.basename(file_name)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading {os.path.basename(file_name)}: {e}\")\n",
    "    \n",
    "    df_lms = pd.concat(all_lms_dfs, ignore_index=True)\n",
    "    \n",
    "    # === EXACT TRAINING FEATURE ENGINEERING ===\n",
    "    \n",
    "    print(\"--- Applying LMS Feature Engineering ---\")\n",
    "    \n",
    "    # Core Cleaning\n",
    "    df_lms['DUE_DATE'] = pd.to_datetime(df_lms['DUE_DATE'], errors='coerce')\n",
    "    df_lms['PAID_DT'] = pd.to_datetime(df_lms['PAID_DT'], errors='coerce')\n",
    "    df_lms.loc[df_lms['STATUS'] == 1, 'PAID_DT'] = pd.NaT\n",
    "    df_lms = df_lms.sort_values(by=['LOAN_ID', 'INSTALLMENT_NO']).reset_index(drop=True)\n",
    "    \n",
    "    # Remove invalid records\n",
    "    df_lms = df_lms.loc[~((df_lms['PAID_DT'].isnull()) & (df_lms['STATUS'] != 1))].reset_index(drop=True)\n",
    "    \n",
    "    # Core Features\n",
    "    df_lms['DAYS_LATE'] = (df_lms['PAID_DT'] - df_lms['DUE_DATE']).dt.days\n",
    "    df_lms['DAYS_BETWEEN_DUE_DATES'] = df_lms.groupby('LOAN_ID')['DUE_DATE'].diff().dt.days\n",
    "    \n",
    "    # Repayment Schedule Category\n",
    "    df_lms['REPAYMENT_SCHEDULE_CAT'] = np.select(\n",
    "        [\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'].isnull(),\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'].isin([28, 29, 30, 31]),\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'] == 7,\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'] == 14,\n",
    "            df_lms['DAYS_BETWEEN_DUE_DATES'] == 56\n",
    "        ],\n",
    "        ['Initial', 'Monthly', 'Weekly', 'Bi-Weekly', 'Bi-Monthly'],\n",
    "        default='Other'\n",
    "    )\n",
    "    \n",
    "    # Loan Schedule Type\n",
    "    has_weekly = df_lms.groupby('LOAN_ID')['REPAYMENT_SCHEDULE_CAT'].transform(\n",
    "        lambda x: 'Weekly' in x.unique())\n",
    "    has_monthly = df_lms.groupby('LOAN_ID')['REPAYMENT_SCHEDULE_CAT'].transform(\n",
    "        lambda x: 'Monthly' in x.unique())\n",
    "    mode_schedule = df_lms.groupby('LOAN_ID')['REPAYMENT_SCHEDULE_CAT'].transform(\n",
    "        lambda x: x.mode()[0] if not x.mode().empty else 'Initial')\n",
    "    df_lms['LOAN_SCHEDULE_TYPE'] = np.where((has_weekly) & (has_monthly), 'Hybrid', mode_schedule)\n",
    "    \n",
    "    # Payment Features\n",
    "    df_lms['IS_UNPAID'] = np.where(df_lms['STATUS'] == 1, 1, 0)\n",
    "    df_lms['IS_DAYS_LATE_MISSING'] = df_lms['DAYS_LATE'].isna().astype(int)\n",
    "    df_lms['DAYS_LATE'] = df_lms['DAYS_LATE'].fillna(0)\n",
    "    df_lms['PAID_RATIO'] = (df_lms['PAID_AMOUNT'] / df_lms['INSTALLMENT_AMOUNT']).clip(upper=1.0)\n",
    "    df_lms['DELTA_DAYS_LATE'] = df_lms.groupby('LOAN_ID')['DAYS_LATE'].diff().fillna(0)\n",
    "    \n",
    "    # CRITICAL: Zero out DELTA_DAYS_LATE for unpaid EMIs (matching training)\n",
    "    df_lms.loc[df_lms['IS_UNPAID'] == 1, 'DELTA_DAYS_LATE'] = 0\n",
    "    \n",
    "    # Current EMI Behavior Label\n",
    "    conditions_behavior = [\n",
    "        (df_lms['STATUS'] == 1) |\n",
    "        ((df_lms['STATUS'] == 0) & (df_lms['DAYS_LATE'] > GRACE_PERIOD_DAYS)) |\n",
    "        ((df_lms['STATUS'] == 2) & (\n",
    "            (df_lms['DAYS_LATE'] > GRACE_PERIOD_DAYS) |\n",
    "            (df_lms['PAID_AMOUNT'] / df_lms['INSTALLMENT_AMOUNT'] < PAID_PERCENTAGE_THRESHOLD)\n",
    "        )),\n",
    "        (df_lms['STATUS'] == 0) & (df_lms['DAYS_LATE'] <= GRACE_PERIOD_DAYS),\n",
    "        (df_lms['STATUS'] == 2) & (df_lms['DAYS_LATE'] <= GRACE_PERIOD_DAYS) &\n",
    "        (df_lms['PAID_AMOUNT'] / df_lms['INSTALLMENT_AMOUNT'] >= PAID_PERCENTAGE_THRESHOLD)\n",
    "    ]\n",
    "    choices_behavior = [1, 0, 0]\n",
    "    df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] = np.select(\n",
    "        conditions_behavior, choices_behavior, default=-1).astype(np.int8)\n",
    "    \n",
    "    # Composite Risk and Payment Score\n",
    "    df_lms['REMAINING_EMI_RATIO'] = 1 - df_lms[\"PAID_RATIO\"]\n",
    "    df_lms['COMPOSITE_RISK'] = df_lms['DAYS_LATE'] + (df_lms['REMAINING_EMI_RATIO'] * 10)\n",
    "    \n",
    "    conditions_score = [\n",
    "        df_lms['IS_UNPAID'] == 1,\n",
    "        (df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] == 1) & (df_lms['IS_UNPAID'] == 0),\n",
    "        (df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] == 0) & (df_lms['DAYS_LATE'] > 0) &\n",
    "        (df_lms['DAYS_LATE'] <= GRACE_PERIOD_DAYS) & (df_lms['IS_UNPAID'] == 0),\n",
    "        (df_lms['CURRENT_EMI_BEHAVIOR_LABEL'] == 0) & (df_lms['DAYS_LATE'] <= 0) &\n",
    "        (df_lms['IS_UNPAID'] == 0)\n",
    "    ]\n",
    "    \n",
    "    choices_score = [\n",
    "        -100,\n",
    "        np.maximum(0.0, np.minimum(0.30, 0.30 - (df_lms['COMPOSITE_RISK'] * 0.03))),\n",
    "        1.0 / (1 + df_lms['COMPOSITE_RISK']),\n",
    "        1.5 + (np.abs(df_lms['DAYS_LATE']) / 10)\n",
    "    ]\n",
    "    df_lms['PAYMENT_SCORE'] = np.select(conditions_score, choices_score, default=0).astype(np.float32)\n",
    "    \n",
    "    rank_choices = [4, 3, 2, 1]\n",
    "    df_lms['PAYMENT_SCORE_RANK'] = np.select(conditions_score, rank_choices).astype(np.int8)\n",
    "    \n",
    "    # === VALIDATION MODE: Extract actual targets before trimming ===\n",
    "    actual_targets = df_lms.groupby('LOAN_ID').tail(TARGET_BACKTEST_EMI).copy()\n",
    "    actual_targets = actual_targets[['LOAN_ID', 'CURRENT_EMI_BEHAVIOR_LABEL']].rename(\n",
    "        columns={'CURRENT_EMI_BEHAVIOR_LABEL': 'ACTUAL_NEXT_EMI_ISSUE'}\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ LMS Feature Engineering Complete. Shape: {df_lms.shape}\")\n",
    "    return df_lms, actual_targets\n",
    "\n",
    "# ===================================================================================\n",
    "# PART 3: DATA MERGING AND FINAL PREPARATION\n",
    "# ===================================================================================\n",
    "\n",
    "def merge_and_finalize_data(df_los, df_lms_full, df_actual_targets, category_mappings):\n",
    "    \"\"\"\n",
    "    Merges LOS and LMS data, applies final feature engineering,\n",
    "    and performs back-test trimming exactly as in training (3_hybrid_data_prep_optimized.ipynb).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. VALIDATION MODE: Trim the last EMI (EMI N) from sequence\n",
    "    print(f\"\\n--- Back-testing Mode: Trimming last {TARGET_BACKTEST_EMI} EMI(s) ---\")\n",
    "    indices_to_drop = df_lms_full.groupby('LOAN_ID').tail(TARGET_BACKTEST_EMI).index\n",
    "    df_lms = df_lms_full.drop(indices_to_drop).reset_index(drop=True)\n",
    "    print(f\"   LMS records after trimming: {df_lms.shape[0]}\")\n",
    "    \n",
    "    # 2. Rename LMS columns to avoid conflicts\n",
    "    RENAME_MAP_LMS = {\n",
    "        'LOAN_AMOUNT': 'LOAN_AMOUNT_LMS',\n",
    "        'TENURE': 'TENURE_LMS',\n",
    "        'INTEREST_RATE': 'INTEREST_RATE_LMS'\n",
    "    }\n",
    "    df_lms.rename(columns=RENAME_MAP_LMS, inplace=True)\n",
    "    \n",
    "    # 3. Rename LOS columns\n",
    "    RENAME_MAP_LOS = {\n",
    "        'LOAN_AMOUNT': 'LOAN_AMOUNT_STATIC',\n",
    "        'TENURE': 'TENURE_STATIC',\n",
    "        'INTEREST_RATE': 'INTEREST_RATE_STATIC'\n",
    "    }\n",
    "    df_los.rename(columns=RENAME_MAP_LOS, inplace=True)\n",
    "    \n",
    "    # 4. Merge\n",
    "    df_combined = pd.merge(df_los, df_lms, on='LOAN_ID', how='inner', suffixes=('_static', '_lms'))\n",
    "    print(f\"   Combined shape after merge: {df_combined.shape}\")\n",
    "    \n",
    "    # 5. Clean up column names\n",
    "    df_combined.rename(columns={\n",
    "        \"LOAN_DATE_static\": \"LOAN_DATE\",\n",
    "        \"LOAN_AMOUNT_STATIC\": \"LOAN_AMOUNT\",\n",
    "        \"TENURE_STATIC\": \"TENURE\",\n",
    "        \"INTEREST_RATE_STATIC\": \"INTEREST_RATE\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # 6. Drop unnecessary columns\n",
    "    cols_to_drop = [\n",
    "        'LOAN_DATE_lms', 'LOAN_AMOUNT_lms', 'TENURE_lms', 'INTEREST_RATE_lms',\n",
    "        'DISBURSED_AMOUNT', 'PIN_CODE', 'IS_DAYS_LATE_MISSING', 'STATUS',\n",
    "        'PAID_DT', 'PAID_AMOUNT', 'REMAINING_EMI_RATIO', 'TENURE', 'INTEREST_RATE'\n",
    "    ]\n",
    "    df_combined.drop(columns=[c for c in cols_to_drop if c in df_combined.columns],\n",
    "                     errors='ignore', inplace=True)\n",
    "    \n",
    "    # 7. Impute DAYS_BETWEEN_DUE_DATES\n",
    "    df_combined[\"DAYS_BETWEEN_DUE_DATES\"].fillna(0, inplace=True)\n",
    "    \n",
    "    # 8. Rolling Feature (RECENT_PAYMENT_SCORE) - WITH PROPER SHIFTING\n",
    "    print(\"   Creating RECENT_PAYMENT_SCORE (rolling window feature)...\")\n",
    "    df_combined['RECENT_PAYMENT_SCORE'] = df_combined.groupby('LOAN_ID')['PAYMENT_SCORE'].rolling(\n",
    "        window=ROLLING_WINDOW_SIZE, min_periods=1\n",
    "    ).mean().reset_index(level=0, drop=True).astype(np.float32)\n",
    "    \n",
    "    # CRITICAL: Shift to prevent leakage\n",
    "    df_combined['RECENT_PAYMENT_SCORE'] = df_combined.groupby('LOAN_ID')['RECENT_PAYMENT_SCORE'].shift(1)\n",
    "    \n",
    "    # Use TRAINING mean for imputation\n",
    "    overall_mean_score = -39.098625  # From training\n",
    "    df_combined['RECENT_PAYMENT_SCORE'].fillna(overall_mean_score, inplace=True)\n",
    "    \n",
    "    # 9. Custom Encoding\n",
    "    print(\"   Applying custom encoding for CUSTOMER_FLAG, OCCUPATION_NAME, LOAN_PURPOSE...\")\n",
    "    df_combined['CUSTOMER_FLAG_ENCODED'] = df_combined['CUSTOMER_FLAG'].map(FLAG_ORDER).fillna(0).astype(np.int8)\n",
    "    \n",
    "    # Use training category mappings for consistent encoding\n",
    "    if category_mappings['occupation'] and category_mappings['purpose']:\n",
    "        occupation_to_code = {v: k+1 for k, v in category_mappings['occupation'].items()}\n",
    "        purpose_to_code = {v: k+1 for k, v in category_mappings['purpose'].items()}\n",
    "        \n",
    "        df_combined['OCCUPATION_NAME_ENCODED'] = df_combined['OCCUPATION_NAME'].astype(str).map(\n",
    "            occupation_to_code).fillna(0).astype(np.int16)\n",
    "        df_combined['LOAN_PURPOSE_ENCODED'] = df_combined['LOAN_PURPOSE'].astype(str).map(\n",
    "            purpose_to_code).fillna(0).astype(np.int16)\n",
    "        \n",
    "        print(f\"      OCCUPATION_NAME: {df_combined['OCCUPATION_NAME_ENCODED'].nunique()} unique codes\")\n",
    "        print(f\"      LOAN_PURPOSE: {df_combined['LOAN_PURPOSE_ENCODED'].nunique()} unique codes\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è WARNING: Using fallback encoding (may be inconsistent)\")\n",
    "    \n",
    "    df_combined.drop(columns=['CUSTOMER_FLAG', 'OCCUPATION_NAME', 'LOAN_PURPOSE', 'LOAN_DATE'],\n",
    "                     errors='ignore', inplace=True)\n",
    "    \n",
    "    # 10. OHE Sequential Categorical Feature\n",
    "    print(\"   One-hot encoding REPAYMENT_SCHEDULE_CAT...\")\n",
    "    df_combined = pd.get_dummies(df_combined, columns=['REPAYMENT_SCHEDULE_CAT'], prefix='REPAYMENT_CAT')\n",
    "    \n",
    "    # 11. Filter for the latest sequence for prediction\n",
    "    print(\"   Filtering to keep only necessary sequence (last MAX_SEQUENCE_LENGTH EMIs)...\")\n",
    "    df_combined['SEQUENCE_COUNT'] = df_combined.groupby('LOAN_ID').cumcount() + 1\n",
    "    df_combined['REVERSE_SEQUENCE_COUNT'] = df_combined.groupby('LOAN_ID')['SEQUENCE_COUNT'].transform(\n",
    "        'max') - df_combined['SEQUENCE_COUNT']\n",
    "    df_combined_filtered = df_combined[df_combined['REVERSE_SEQUENCE_COUNT'] < MAX_SEQUENCE_LENGTH].copy()\n",
    "    df_combined_filtered.drop(columns=['SEQUENCE_COUNT', 'REVERSE_SEQUENCE_COUNT'], inplace=True)\n",
    "    \n",
    "    # 12. Merge Actual Target for Validation\n",
    "    df_combined_filtered = pd.merge(df_combined_filtered, df_actual_targets, on='LOAN_ID', how='left')\n",
    "    df_combined_filtered['ACTUAL_NEXT_EMI_ISSUE'] = df_combined_filtered.groupby(\n",
    "        'LOAN_ID')['ACTUAL_NEXT_EMI_ISSUE'].transform('max')\n",
    "    df_combined_filtered.dropna(subset=['ACTUAL_NEXT_EMI_ISSUE'], inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ Data preparation complete. Final shape: {df_combined_filtered.shape}\")\n",
    "    print(f\"   Unique loans: {df_combined_filtered['LOAN_ID'].nunique()}\")\n",
    "    \n",
    "    return df_combined_filtered\n",
    "\n",
    "# ===================================================================================\n",
    "# PART 4: PREPROCESSING AND PREDICTION\n",
    "# ===================================================================================\n",
    "\n",
    "def preprocess_and_predict(df_full, preprocessor, model):\n",
    "    \"\"\"\n",
    "    Applies preprocessor, reshapes sequences, and runs inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n--- Step: Preprocessing and Prediction ---\")\n",
    "\n",
    "    REPAYMENT_CAT_OHE_COLS = [c for c in df_full.columns if c.startswith('REPAYMENT_CAT_')]\n",
    "    \n",
    "    # Add CUSTOMER_FLAG_ENCODED to static numerical features\n",
    "    STATIC_COLS_NUMERICAL_WITH_FLAG = STATIC_COLS_NUMERICAL + ['CUSTOMER_FLAG_ENCODED']\n",
    "    \n",
    "    # Reconstruct NUMERICAL_FEATURES_FINAL exactly as in training\n",
    "    NUMERICAL_FEATURES_FINAL = SEQUENTIAL_COLS_NUMERICAL + \\\n",
    "                               STATIC_COLS_NUMERICAL_WITH_FLAG + \\\n",
    "                               REPAYMENT_CAT_OHE_COLS\n",
    "    \n",
    "    print(f\"   Reconstructed NUMERICAL_FEATURES_FINAL with {len(NUMERICAL_FEATURES_FINAL)} features\")\n",
    "    print(f\"      - Sequential: {len(SEQUENTIAL_COLS_NUMERICAL)}\")\n",
    "    print(f\"      - Static Numerical: {len(STATIC_COLS_NUMERICAL_WITH_FLAG)}\")\n",
    "    print(f\"      - Repayment OHE: {len(REPAYMENT_CAT_OHE_COLS)}\")\n",
    "    \n",
    "    # Get expected features from preprocessor\n",
    "    expected_features = list(preprocessor.feature_names_in_)\n",
    "    \n",
    "    # Align DataFrame columns with expected features (safe approach)\n",
    "    df_aligned = df_full.reindex(columns=expected_features, fill_value=0)\n",
    "    print(f\"   DataFrame aligned to {len(expected_features)} expected features\")\n",
    "    \n",
    "    # Apply ColumnTransformer\n",
    "    print(\"   Applying StandardScaler and OneHotEncoder...\")\n",
    "    X_scaled_ohe = preprocessor.transform(df_aligned)\n",
    "    ALL_FINAL_COLS = list(preprocessor.get_feature_names_out())\n",
    "\n",
    "    # Reconstruct DataFrame\n",
    "    X_predict_df = pd.DataFrame(X_scaled_ohe, columns=ALL_FINAL_COLS, index=df_full.index)\n",
    "    \n",
    "    # Add back non-transformed columns\n",
    "    X_predict_df['LOAN_ID'] = df_full['LOAN_ID'].values\n",
    "    X_predict_df['ACTUAL_NEXT_EMI_ISSUE'] = df_full['ACTUAL_NEXT_EMI_ISSUE'].values\n",
    "    for col in STATIC_EMBEDDING_COLS:\n",
    "        X_predict_df[col] = df_full[col].values\n",
    "    # ============================================================\n",
    "    # ‚úÖ STEP 5: CORRECTED - Define input feature sets\n",
    "    # ============================================================\n",
    "    \n",
    "    # Build a set of exact feature names we want to include/exclude\n",
    "    # This prevents partial string matching bugs\n",
    "    \n",
    "    # 5A. LSTM Features (Sequential + Repayment OHE)\n",
    "    lstm_sequential_features = [f\"num__{col}\" for col in SEQUENTIAL_COLS_NUMERICAL]\n",
    "    lstm_repayment_features = [c for c in ALL_FINAL_COLS if c.startswith('num__REPAYMENT_CAT_')]\n",
    "    LSTM_INPUT_COLS_FINAL = lstm_sequential_features + lstm_repayment_features\n",
    "    \n",
    "    # 5B. Static Dense Features (Numerical + OHE)\n",
    "    # ‚úÖ CORRECTED: Use exact column name matching to avoid substring bugs\n",
    "    static_numerical_features = [f\"num__{col}\" for col in STATIC_COLS_NUMERICAL_WITH_FLAG]\n",
    "    \n",
    "    # Get OHE features - these have 'cat__' prefix from the ColumnTransformer\n",
    "    static_ohe_features = [c for c in ALL_FINAL_COLS \n",
    "                          if any(c.startswith(f'cat__{base}') for base in STATIC_COLS_OHE)]\n",
    "    \n",
    "    STATIC_DENSE_COLS_FINAL = static_numerical_features + static_ohe_features\n",
    "    # ============================================================\n",
    "    # ‚úÖ STEP 6: CRITICAL VALIDATION - Verify all features exist\n",
    "    # ============================================================\n",
    "    \n",
    "    # Check if all expected features are in the transformed data\n",
    "    missing_lstm = [f for f in LSTM_INPUT_COLS_FINAL if f not in ALL_FINAL_COLS]\n",
    "    missing_static = [f for f in STATIC_DENSE_COLS_FINAL if f not in ALL_FINAL_COLS]\n",
    "    \n",
    "    if missing_lstm:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: {len(missing_lstm)} LSTM features not found in transformed data:\")\n",
    "        print(f\"      {missing_lstm[:5]}...\")  # Show first 5\n",
    "        # Filter out missing features\n",
    "        LSTM_INPUT_COLS_FINAL = [f for f in LSTM_INPUT_COLS_FINAL if f in ALL_FINAL_COLS]\n",
    "    \n",
    "    if missing_static:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: {len(missing_static)} Static features not found in transformed data:\")\n",
    "        print(f\"      {missing_static[:5]}...\")\n",
    "        STATIC_DENSE_COLS_FINAL = [f for f in STATIC_DENSE_COLS_FINAL if f in ALL_FINAL_COLS]\n",
    "    # ============================================================\n",
    "    # STEP 7: Print dimensions for verification\n",
    "    # ============================================================\n",
    "    print(f\"\\n   üìä Feature Breakdown:\")\n",
    "    print(f\"   LSTM input features: {len(LSTM_INPUT_COLS_FINAL)}\")\n",
    "    print(f\"      - Sequential: {len(lstm_sequential_features)}\")\n",
    "    print(f\"      - Repayment OHE: {len(lstm_repayment_features)}\")\n",
    "    \n",
    "    print(f\"   Static dense features: {len(STATIC_DENSE_COLS_FINAL)}\")\n",
    "    print(f\"      - Numerical: {len(static_numerical_features)}\")\n",
    "    print(f\"      - OHE: {len(static_ohe_features)}\")\n",
    "    \n",
    "    print(f\"   Embedding features: {len(STATIC_EMBEDDING_COLS)}\")\n",
    "    \n",
    "    # Sequence Reshaping\n",
    "    def reshape_for_prediction(X_df, lstm_cols, static_dense_cols, embedding_cols, max_len):\n",
    "        grouped = X_df.groupby('LOAN_ID')\n",
    "        loan_ids = list(grouped.groups.keys())\n",
    "        \n",
    "        X_lstm = np.zeros((len(loan_ids), max_len, len(lstm_cols)), dtype=np.float32)\n",
    "        X_static_dense = np.zeros((len(loan_ids), len(static_dense_cols)), dtype=np.float32)\n",
    "        X_static_embed = np.zeros((len(loan_ids), len(embedding_cols)), dtype=np.int16)\n",
    "        y_actual = np.zeros((len(loan_ids),), dtype=np.int8)\n",
    "        \n",
    "        for i, loan_id in enumerate(loan_ids):\n",
    "            loan_data = grouped.get_group(loan_id)\n",
    "            sequence = loan_data[lstm_cols].values\n",
    "            \n",
    "            # Pad or truncate sequence\n",
    "            if len(sequence) >= max_len:\n",
    "                X_lstm[i, :, :] = sequence[-max_len:]\n",
    "            else:\n",
    "                X_lstm[i, -len(sequence):, :] = sequence\n",
    "            \n",
    "            # Get static features from last record\n",
    "            last_record = loan_data.iloc[-1]\n",
    "            X_static_dense[i, :] = last_record[static_dense_cols].values\n",
    "            X_static_embed[i, :] = last_record[embedding_cols].values.astype(np.int16)\n",
    "            y_actual[i] = last_record['ACTUAL_NEXT_EMI_ISSUE'].astype(np.int8)\n",
    "        \n",
    "        return X_lstm, X_static_dense, X_static_embed, loan_ids, y_actual\n",
    "    \n",
    "    # Execute reshaping\n",
    "    print(\"   Reshaping sequences for model input...\")\n",
    "    X_predict_lstm, X_predict_static_dense, X_predict_static_embed, prediction_loan_ids, y_actual = \\\n",
    "        reshape_for_prediction(X_predict_df, LSTM_INPUT_COLS_FINAL, STATIC_DENSE_COLS_FINAL,\n",
    "                              STATIC_EMBEDDING_COLS, MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    print(f\"   Reshaped data shapes:\")\n",
    "    print(f\"      LSTM: {X_predict_lstm.shape}\")\n",
    "    print(f\"      Static Dense: {X_predict_static_dense.shape}\")\n",
    "    print(f\"      Embedding: {X_predict_static_embed.shape}\")\n",
    "\n",
    "    print(f\"\\n   üîç Dimension Verification:\")\n",
    "    print(f\"   From training notebook, your shapes should be:\")\n",
    "    print(f\"      LSTM: (n_loans, 8, {12 + len(REPAYMENT_CAT_OHE_COLS)})\")  # 12 sequential + OHE cols\n",
    "    print(f\"      Static Dense: (n_loans, {6 + len(static_ohe_features)})\")  # 6 numerical + OHE\n",
    "    print(f\"      Embedding: (n_loans, 2)\")\n",
    "\n",
    "    # Expected dimensions (get these from your training output)\n",
    "    EXPECTED_LSTM_FEATURES = 12 + len(REPAYMENT_CAT_OHE_COLS)  # Sequential + Repayment OHE\n",
    "    EXPECTED_STATIC_FEATURES = 6 + len(static_ohe_features)     # Numerical + State/Marital/Loan_Schedule OHE\n",
    "    \n",
    "    if X_predict_lstm.shape[2] != EXPECTED_LSTM_FEATURES:\n",
    "        print(f\"   ‚ùå LSTM MISMATCH! Expected {EXPECTED_LSTM_FEATURES}, got {X_predict_lstm.shape[2]}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ LSTM dimensions match!\")\n",
    "    \n",
    "    if X_predict_static_dense.shape[1] != EXPECTED_STATIC_FEATURES:\n",
    "        print(f\"   ‚ùå STATIC MISMATCH! Expected {EXPECTED_STATIC_FEATURES}, got {X_predict_static_dense.shape[1]}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Static Dense dimensions match!\")\n",
    "\n",
    "    \n",
    "    # Model Inference\n",
    "    print(\"\\n   Running model inference...\")\n",
    "    prediction_inputs = {\n",
    "        'lstm_input': X_predict_lstm,\n",
    "        'static_dense_input': X_predict_static_dense,\n",
    "        'embedding_input': X_predict_static_embed\n",
    "    }\n",
    "    \n",
    "    y_pred_proba = model.predict(prediction_inputs, verbose=0)[:, 0]\n",
    "    y_pred_class = (y_pred_proba > 0.5).astype(np.int8)\n",
    "    print(f\"‚úÖ Prediction complete for {len(prediction_loan_ids)} unique loans.\")\n",
    "    return prediction_loan_ids, y_pred_proba, y_pred_class, y_actual, prediction_inputs, LSTM_INPUT_COLS_FINAL, STATIC_DENSE_COLS_FINAL\n",
    "\n",
    "# ===================================================================================\n",
    "# PART 5: EVALUATION AND REPORTING\n",
    "# ===================================================================================\n",
    "\n",
    "def evaluate_performance(y_true, y_pred_class, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Generates a professional performance report for the back-test.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìà BACK-TEST PERFORMANCE REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred_class)\n",
    "    cm = confusion_matrix(y_true, y_pred_class)\n",
    "    \n",
    "    print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(pd.DataFrame(cm, \n",
    "                       index=['Actual Paid', 'Actual Default'], \n",
    "                       columns=['Pred Paid', 'Pred Default']))\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred_class, target_names=['Paid (0)', 'Not Paid (1)']))\n",
    "    \n",
    "    # Senior insight: Financial models care about the \"Cost of False Negatives\"\n",
    "    fn = cm[1, 0]\n",
    "    print(f\"‚ö†Ô∏è  Missed Defaults (False Negatives): {fn}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "def calculate_feature_importance_corrected(model, inputs_dict, y_true, y_pred_proba_baseline, \n",
    "                                           lstm_cols, static_cols, embed_cols):\n",
    "    \"\"\"\n",
    "    Calculates feature importance using DROP-COLUMN method with AUC metric.\n",
    "    This matches the training methodology exactly.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä FEATURE IMPORTANCE CALCULATION (Drop-Column Method with AUC)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate baseline AUC\n",
    "    baseline_auc = roc_auc_score(y_true, y_pred_proba_baseline)\n",
    "    print(f\"Baseline Test AUC: {baseline_auc:.4f}\\n\")\n",
    "    \n",
    "    importance_results = []\n",
    "    \n",
    "    # === LSTM Features (Sequential) ===\n",
    "    print(\"Calculating importance for LSTM features...\")\n",
    "    for i, col_name in enumerate(lstm_cols):\n",
    "        # Create corrupted copy\n",
    "        inputs_corrupted = {k: v.copy() for k, v in inputs_dict.items()}\n",
    "        \n",
    "        # ‚úÖ FIXED: Zero out the feature across all timesteps (matching training)\n",
    "        inputs_corrupted['lstm_input'][:, :, i] = 0\n",
    "        \n",
    "        # Predict with corrupted data\n",
    "        y_pred_corrupted = model.predict(inputs_corrupted, verbose=0)[:, 0]\n",
    "        corrupted_auc = roc_auc_score(y_true, y_pred_corrupted)\n",
    "        \n",
    "        # Calculate importance as AUC drop\n",
    "        importance = baseline_auc - corrupted_auc\n",
    "        \n",
    "        importance_results.append({\n",
    "            'Feature': col_name.replace('num__', ''),\n",
    "            'Importance': importance,\n",
    "            'Type': 'Sequential (LSTM)',\n",
    "            'Baseline_AUC': baseline_auc,\n",
    "            'Corrupted_AUC': corrupted_auc\n",
    "        })\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(f\"   Processed {i+1}/{len(lstm_cols)} LSTM features...\")\n",
    "    \n",
    "    # === Static Dense Features ===\n",
    "    print(\"\\nCalculating importance for Static Dense features...\")\n",
    "    for i, col_name in enumerate(static_cols):\n",
    "        inputs_corrupted = {k: v.copy() for k, v in inputs_dict.items()}\n",
    "        \n",
    "        # ‚úÖ FIXED: Zero out the feature\n",
    "        inputs_corrupted['static_dense_input'][:, i] = 0\n",
    "        \n",
    "        y_pred_corrupted = model.predict(inputs_corrupted, verbose=0)[:, 0]\n",
    "        corrupted_auc = roc_auc_score(y_true, y_pred_corrupted)\n",
    "        \n",
    "        importance = baseline_auc - corrupted_auc\n",
    "        \n",
    "        # Determine if it's OHE or numerical\n",
    "        feature_type = 'Static (OHE)' if col_name.startswith('cat__') else 'Static (Numerical)'\n",
    "        \n",
    "        importance_results.append({\n",
    "            'Feature': col_name.replace('num__', '').replace('cat__', ''),\n",
    "            'Importance': importance,\n",
    "            'Type': feature_type,\n",
    "            'Baseline_AUC': baseline_auc,\n",
    "            'Corrupted_AUC': corrupted_auc\n",
    "        })\n",
    "    \n",
    "    # === Embedding Features ===\n",
    "    print(\"\\nCalculating importance for Embedding features...\")\n",
    "    for i, col_name in enumerate(embed_cols):\n",
    "        inputs_corrupted = {k: v.copy() for k, v in inputs_dict.items()}\n",
    "        \n",
    "        # ‚úÖ FIXED: Set to 0 (unknown category)\n",
    "        inputs_corrupted['embedding_input'][:, i] = 0\n",
    "        \n",
    "        y_pred_corrupted = model.predict(inputs_corrupted, verbose=0)[:, 0]\n",
    "        corrupted_auc = roc_auc_score(y_true, y_pred_corrupted)\n",
    "        \n",
    "        importance = baseline_auc - corrupted_auc\n",
    "        \n",
    "        importance_results.append({\n",
    "            'Feature': col_name.replace('_ENCODED', ''),\n",
    "            'Importance': importance,\n",
    "            'Type': 'Static (Embedding)',\n",
    "            'Baseline_AUC': baseline_auc,\n",
    "            'Corrupted_AUC': corrupted_auc\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and sort\n",
    "    importance_df = pd.DataFrame(importance_results)\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Feature Importance Calculation Complete\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "# MAIN EXECUTION PIPELINE\n",
    "# ===================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 1. Load Model and Artifacts\n",
    "        preprocessor, model, category_mappings = load_artifacts()\n",
    "\n",
    "        # 2. Extract Static Data (LOS)\n",
    "        df_los = load_and_preprocess_los()\n",
    "\n",
    "        # 3. Extract Sequential Data (LMS)\n",
    "        df_lms_full, df_actual_targets = load_and_engineer_lms()\n",
    "\n",
    "        # 4. Merge and Align for Production\n",
    "        df_final = merge_and_finalize_data(df_los, df_lms_full, df_actual_targets, category_mappings)\n",
    "\n",
    "        # 5. Inference (‚úÖ Now returns additional data for feature importance)\n",
    "        loan_ids, probabilities, classes, actuals, prediction_inputs, lstm_cols, static_cols = \\\n",
    "            preprocess_and_predict(df_final, preprocessor, model)\n",
    "\n",
    "        # 6. Performance Evaluation (Back-testing)\n",
    "        evaluate_performance(actuals, classes, probabilities)\n",
    "\n",
    "        # 7. Consolidated Results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'LOAN_ID': loan_ids,\n",
    "            'ACTUAL_VALUE': actuals,\n",
    "            'PREDICTED_CLASS': classes,\n",
    "            'DEFAULT_PROBABILITY': probabilities\n",
    "        })\n",
    "        \n",
    "        # Mapping for readability\n",
    "        results_df['PREDICTED_STATUS'] = results_df['PREDICTED_CLASS'].map(TWO_CLASS_STATUS_MAP)\n",
    "        results_df['ACTUAL_STATUS'] = results_df['ACTUAL_VALUE'].map(TWO_CLASS_STATUS_MAP)\n",
    "\n",
    "        # 8. Save results\n",
    "        output_file = \"oct_2025_backtest_results.csv\"\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"‚úÖ Results successfully exported to: {output_file}\")\n",
    "        \n",
    "        # ================================================================\n",
    "        # 9. ‚úÖ CORRECTED FEATURE IMPORTANCE ANALYSIS\n",
    "        # ================================================================\n",
    "        \n",
    "        print(\"\\n\" + \"üîç\"*35)\n",
    "        print(\"STARTING FEATURE IMPORTANCE ANALYSIS\")\n",
    "        print(\"üîç\"*35 + \"\\n\")\n",
    "        \n",
    "        # Calculate feature importance with corrected method\n",
    "        importance_df = calculate_feature_importance_corrected(\n",
    "            model=model,\n",
    "            inputs_dict=prediction_inputs,\n",
    "            y_true=actuals,\n",
    "            y_pred_proba_baseline=probabilities,\n",
    "            lstm_cols=lstm_cols,\n",
    "            static_cols=static_cols,\n",
    "            embed_cols=STATIC_EMBEDDING_COLS\n",
    "        )\n",
    "        \n",
    "        # Display Top 20 Features\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üèÜ TOP 20 MOST IMPORTANT FEATURES\")\n",
    "        print(\"=\"*70)\n",
    "        print(importance_df[['Feature', 'Type', 'Importance', 'Corrupted_AUC']].head(20).to_string(index=False))\n",
    "        \n",
    "        # Save importance results\n",
    "        importance_df.to_csv('feature_importance_results.csv', index=False)\n",
    "        print(f\"\\n‚úÖ Feature importance saved to: feature_importance_results.csv\")\n",
    "        \n",
    "        # ‚úÖ CORRECTED VISUALIZATION\n",
    "        print(\"\\nüìä Generating feature importance visualization...\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Get top 20 features\n",
    "        top_features = importance_df.head(20)\n",
    "        \n",
    "        # Create color map based on feature type\n",
    "        type_colors = {\n",
    "            'Sequential (LSTM)': '#2E86AB',\n",
    "            'Static (Numerical)': '#A23B72',\n",
    "            'Static (OHE)': '#F18F01',\n",
    "            'Static (Embedding)': '#C73E1D'\n",
    "        }\n",
    "        colors = [type_colors.get(t, '#808080') for t in top_features['Type']]\n",
    "        \n",
    "        # Create horizontal bar plot\n",
    "        bars = plt.barh(range(len(top_features)), top_features['Importance'], color=colors)\n",
    "        plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "        plt.xlabel('Importance (AUC Drop when Feature Removed)', fontsize=12, fontweight='bold')\n",
    "        plt.title('Top 20 Feature Importance - EMI Default Prediction\\n(Drop-Column Method with AUC Metric)', \n",
    "                  fontsize=14, fontweight='bold', pad=20)\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor=color, label=label) \n",
    "                          for label, color in type_colors.items()]\n",
    "        plt.legend(handles=legend_elements, loc='lower right', frameon=True, shadow=True)\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig('feature_importance_plot.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úÖ Feature importance plot saved to: feature_importance_plot.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # ‚úÖ ADDITIONAL ANALYSIS: Feature importance by type\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìà FEATURE IMPORTANCE SUMMARY BY TYPE\")\n",
    "        print(\"=\"*70)\n",
    "        type_summary = importance_df.groupby('Type')['Importance'].agg(['count', 'mean', 'sum', 'max'])\n",
    "        type_summary.columns = ['Count', 'Avg Importance', 'Total Importance', 'Max Importance']\n",
    "        print(type_summary.to_string())\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ PIPELINE EXECUTION COMPLETE\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå PIPELINE FAILED: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e87d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TOTAL_INCOME', 'TOTAL_EXPENSE', 'LOAN_AMOUNT', 'AGE', 'CYCLE']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATIC_COLS_NUMERICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6496bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_local_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
